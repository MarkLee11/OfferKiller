# Step 4: Kubernetes Foundational Platform Services Deployment

## Overview
Deploy foundational platform services using Kubernetes, including Nacos service registry, configuration management, and service mesh components. This step establishes the core infrastructure for microservices communication, service discovery, and configuration management.

## Prerequisites Checklist
Before starting, ensure you have:
- [ ] Step 1 completed (Docker environment running on Linux VM)
- [ ] Step 2 completed (Git monorepo structure initialized on Windows)
- [ ] Step 3 completed (GitHub Actions CI/CD pipeline setup)
- [ ] Linux VM with at least 16GB RAM and 100GB disk space
- [ ] Kubernetes cluster access (we'll set up minikube for development)
- [ ] kubectl CLI tool installed on both Windows and Linux
- [ ] Helm package manager installed
- [ ] Network connectivity between Windows and Linux VM

## What We'll Create
```
infrastructure/kubernetes/
‚îú‚îÄ‚îÄ foundational/                      # Core platform services
‚îÇ   ‚îú‚îÄ‚îÄ namespace.yaml                 # Kubernetes namespaces
‚îÇ   ‚îú‚îÄ‚îÄ nacos/                         # Service registry and config
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nacos-deployment.yaml      # Nacos server deployment
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nacos-service.yaml         # Nacos service exposure
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nacos-configmap.yaml       # Nacos configuration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ nacos-pvc.yaml             # Persistent volume claims
‚îÇ   ‚îú‚îÄ‚îÄ istio/                         # Service mesh
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ istio-installation.yaml    # Istio base installation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gateway.yaml               # Ingress gateway
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ virtual-services.yaml      # Traffic routing
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ destination-rules.yaml     # Load balancing rules
‚îÇ   ‚îú‚îÄ‚îÄ monitoring/                    # Observability stack
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prometheus/                # Metrics collection
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ grafana/                   # Dashboards
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ jaeger/                    # Distributed tracing
‚îÇ   ‚îî‚îÄ‚îÄ security/                      # Security policies
‚îÇ       ‚îú‚îÄ‚îÄ rbac.yaml                  # Role-based access control
‚îÇ       ‚îú‚îÄ‚îÄ network-policies.yaml      # Network security
‚îÇ       ‚îî‚îÄ‚îÄ pod-security.yaml          # Pod security standards
‚îú‚îÄ‚îÄ applications/                      # Application deployments
‚îÇ   ‚îú‚îÄ‚îÄ backend/                       # Java microservices
‚îÇ   ‚îî‚îÄ‚îÄ ai-services/                   # Python AI services
‚îî‚îÄ‚îÄ environments/                      # Environment-specific configs
    ‚îú‚îÄ‚îÄ development/                   # Development overlays
    ‚îú‚îÄ‚îÄ staging/                       # Staging configurations
    ‚îî‚îÄ‚îÄ production/                    # Production configurations
```

## Platform Architecture
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        Kubernetes Cluster                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ   Istio Mesh    ‚îÇ  ‚îÇ  Nacos Service  ‚îÇ  ‚îÇ   Monitoring    ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ   (Traffic)     ‚îÇ  ‚îÇ   Registry      ‚îÇ  ‚îÇ    Stack        ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ                 ‚îÇ  ‚îÇ                 ‚îÇ  ‚îÇ                 ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Gateway       ‚îÇ  ‚îÇ ‚Ä¢ Discovery     ‚îÇ  ‚îÇ ‚Ä¢ Prometheus    ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ VirtualSvc    ‚îÇ  ‚îÇ ‚Ä¢ Config Mgmt   ‚îÇ  ‚îÇ ‚Ä¢ Grafana       ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ DestRule      ‚îÇ  ‚îÇ ‚Ä¢ Health Check  ‚îÇ  ‚îÇ ‚Ä¢ Jaeger        ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ           ‚îÇ                     ‚îÇ                     ‚îÇ            ‚îÇ
‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îÇ                                 ‚îÇ                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ                  Application Layer                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ User Service‚îÇ  ‚îÇ Job Service ‚îÇ  ‚îÇ AI Services ‚îÇ        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ   (Java)    ‚îÇ  ‚îÇ   (Java)    ‚îÇ  ‚îÇ  (Python)   ‚îÇ        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Step-by-Step Implementation

### 4.1 Linux VM: Install Kubernetes (Minikube for Development)

**SSH into your Linux VM and run these commands:**

```bash
# Update system packages
sudo yum update -y  # For CentOS/RHEL
# OR
sudo apt update && sudo apt upgrade -y  # For Ubuntu/Debian

# Install curl if not present
sudo yum install -y curl  # For CentOS/RHEL
# OR
sudo apt install -y curl  # For Ubuntu/Debian
```

#### 4.1.1 Install kubectl

```bash
# Download kubectl
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"

# Make kubectl executable
chmod +x kubectl

# Move to system path
sudo mv kubectl /usr/local/bin/

# Verify installation
kubectl version --client
```

#### 4.1.2 Install Minikube

```bash
# Download minikube
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64

# Install minikube
sudo install minikube-linux-amd64 /usr/local/bin/minikube

# Verify installation
minikube version
```

#### 4.1.3 Start Minikube Cluster

```bash
# Start minikube with adequate resources for our services
minikube start \
  --driver=docker \
  --cpus=4 \
  --memory=8192 \
  --disk-size=50gb \
  --kubernetes-version=v1.28.0

# Wait for cluster to be ready
kubectl get nodes

# Enable required addons
minikube addons enable ingress
minikube addons enable metrics-server
minikube addons enable dashboard

# Verify cluster status
kubectl cluster-info
```

### 4.2 Linux VM: Install Helm Package Manager

```bash
# Download and install Helm
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# Add required Helm repositories
helm repo add stable https://charts.helm.sh/stable
helm repo add istio https://istio-release.storage.googleapis.com/charts
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo add grafana https://grafana.github.io/helm-charts
helm repo add jaegertracing https://jaegertracing.github.io/helm-charts

# Update repositories
helm repo update

# Verify installation
helm version
```

### 4.3 Windows: Install kubectl and Helm for Local Development

**On your Windows PC:**

#### 4.3.1 Install kubectl on Windows

**Using PowerShell as Administrator:**

```powershell
# Download kubectl
curl.exe -LO "https://dl.k8s.io/release/v1.28.0/bin/windows/amd64/kubectl.exe"

# Move to a directory in your PATH (create if it doesn't exist)
New-Item -ItemType Directory -Force -Path "C:\kubectl"
Move-Item .\kubectl.exe C:\kubectl\

# Add to PATH (permanent)
[Environment]::SetEnvironmentVariable("Path", $env:Path + ";C:\kubectl", [EnvironmentVariableTarget]::Machine)

# Restart PowerShell and verify
kubectl version --client
```

#### 4.3.2 Install Helm on Windows

```powershell
# Download Helm
Invoke-WebRequest -Uri "https://get.helm.sh/helm-v3.13.0-windows-amd64.zip" -OutFile "helm.zip"

# Extract
Expand-Archive -Path "helm.zip" -DestinationPath "C:\helm"

# Add to PATH
[Environment]::SetEnvironmentVariable("Path", $env:Path + ";C:\helm\windows-amd64", [EnvironmentVariableTarget]::Machine)

# Restart PowerShell and verify
helm version
```

### 4.4 Windows: Create Kubernetes Configuration Files

**Navigate to your project directory and create the Kubernetes configurations:**

#### 4.4.1 Create Namespace Configuration

**Create file: `E:\OfferKiller\infrastructure\kubernetes\foundational\namespace.yaml`**

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: offerkiller-system
  labels:
    name: offerkiller-system
    purpose: foundational-services
    istio-injection: enabled
---
apiVersion: v1
kind: Namespace
metadata:
  name: offerkiller-app
  labels:
    name: offerkiller-app
    purpose: application-services
    istio-injection: enabled
---
apiVersion: v1
kind: Namespace
metadata:
  name: offerkiller-monitoring
  labels:
    name: offerkiller-monitoring
    purpose: monitoring-services
    istio-injection: disabled
```

#### 4.4.2 Create Nacos Service Registry Configuration

**Create file: `E:\OfferKiller\infrastructure\kubernetes\foundational\nacos\nacos-configmap.yaml`**

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nacos-config
  namespace: offerkiller-system
data:
  application.properties: |
    # Nacos Configuration
    server.servlet.contextPath=/nacos
    server.port=8848
    
    # Database Configuration
    spring.datasource.platform=mysql
    db.num=1
    db.url.0=jdbc:mysql://mysql:3306/nacos?characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true&useUnicode=true&useSSL=false&serverTimezone=UTC
    db.user.0=nacos
    db.password.0=nacos123
    
    # Management Configuration
    management.endpoints.web.exposure.include=*
    management.metrics.export.elastic.enabled=false
    management.metrics.export.influx.enabled=false
    
    # Nacos Security
    nacos.security.ignore.urls=/,/error,/**/*.css,/**/*.js,/**/*.html,/**/*.map,/**/*.svg,/**/*.png,/**/*.ico,/console-fe/public/**,/v1/auth/**,/v1/console/health/**,/actuator/**,/v1/console/namespace/**
    
    # Cluster Configuration
    nacos.naming.distro.taskDispatchThreadCount=10
    nacos.naming.distro.taskDispatchPeriod=200
    nacos.naming.distro.batchSyncKeyCount=1000
    nacos.naming.distro.initDataRatio=0.9
    nacos.naming.distro.syncRetryDelay=5000
    nacos.naming.data.warmup=true
    nacos.naming.expireInstance=true
```

**Create file: `E:\OfferKiller\infrastructure\kubernetes\foundational\nacos\nacos-pvc.yaml`**

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nacos-data-pvc
  namespace: offerkiller-system
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: standard
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nacos-logs-pvc
  namespace: offerkiller-system
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: standard
```

**Create file: `E:\OfferKiller\infrastructure\kubernetes\foundational\nacos\nacos-deployment.yaml`**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nacos
  namespace: offerkiller-system
  labels:
    app: nacos
    version: v2.3.0
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nacos
  template:
    metadata:
      labels:
        app: nacos
        version: v2.3.0
      annotations:
        sidecar.istio.io/inject: "true"
    spec:
      containers:
      - name: nacos
        image: nacos/nacos-server:v2.3.0
        ports:
        - containerPort: 8848
          name: http
        - containerPort: 9848
          name: grpc
        - containerPort: 9849
          name: rpc
        env:
        - name: MODE
          value: "standalone"
        - name: PREFER_HOST_MODE
          value: "hostname"
        - name: SPRING_DATASOURCE_PLATFORM
          value: "mysql"
        - name: MYSQL_SERVICE_HOST
          value: "mysql"
        - name: MYSQL_SERVICE_PORT
          value: "3306"
        - name: MYSQL_SERVICE_DB_NAME
          value: "nacos"
        - name: MYSQL_SERVICE_USER
          value: "nacos"
        - name: MYSQL_SERVICE_PASSWORD
          value: "nacos123"
        - name: NACOS_APPLICATION_PORT
          value: "8848"
        - name: NACOS_SERVER_IP
          value: "0.0.0.0"
        volumeMounts:
        - name: nacos-data
          mountPath: /home/nacos/data
        - name: nacos-logs
          mountPath: /home/nacos/logs
        - name: nacos-config
          mountPath: /home/nacos/conf/application.properties
          subPath: application.properties
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /nacos/actuator/health
            port: 8848
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
        readinessProbe:
          httpGet:
            path: /nacos/actuator/health
            port: 8848
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
      volumes:
      - name: nacos-data
        persistentVolumeClaim:
          claimName: nacos-data-pvc
      - name: nacos-logs
        persistentVolumeClaim:
          claimName: nacos-logs-pvc
      - name: nacos-config
        configMap:
          name: nacos-config
---
apiVersion: v1
kind: Service
metadata:
  name: nacos
  namespace: offerkiller-system
  labels:
    app: nacos
spec:
  type: ClusterIP
  ports:
  - port: 8848
    targetPort: 8848
    protocol: TCP
    name: http
  - port: 9848
    targetPort: 9848
    protocol: TCP
    name: grpc
  - port: 9849
    targetPort: 9849
    protocol: TCP
    name: rpc
  selector:
    app: nacos
---
apiVersion: v1
kind: Service
metadata:
  name: nacos-nodeport
  namespace: offerkiller-system
  labels:
    app: nacos
spec:
  type: NodePort
  ports:
  - port: 8848
    targetPort: 8848
    protocol: TCP
    name: http
    nodePort: 30848
  selector:
    app: nacos
```

#### 4.4.3 Create MySQL Database for Nacos

**Create file: `E:\OfferKiller\infrastructure\kubernetes\foundational\nacos\mysql-nacos.yaml`**

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: mysql-nacos-secret
  namespace: offerkiller-system
type: Opaque
data:
  mysql-root-password: cm9vdDEyMw==  # root123
  mysql-password: bmFjb3MxMjM=        # nacos123
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: mysql-nacos-config
  namespace: offerkiller-system
data:
  nacos-init.sql: |
    CREATE DATABASE IF NOT EXISTS nacos CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;
    CREATE USER IF NOT EXISTS 'nacos'@'%' IDENTIFIED BY 'nacos123';
    GRANT ALL PRIVILEGES ON nacos.* TO 'nacos'@'%';
    
    USE nacos;
    
    CREATE TABLE IF NOT EXISTS config_info (
      id bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'id',
      data_id varchar(255) NOT NULL COMMENT 'data_id',
      group_id varchar(128) DEFAULT NULL,
      content longtext NOT NULL COMMENT 'content',
      md5 varchar(32) DEFAULT NULL COMMENT 'md5',
      gmt_create datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT 'ÂàõÂª∫Êó∂Èó¥',
      gmt_modified datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '‰øÆÊîπÊó∂Èó¥',
      src_user text COMMENT 'source user',
      src_ip varchar(50) DEFAULT NULL COMMENT 'source ip',
      app_name varchar(128) DEFAULT NULL,
      tenant_id varchar(128) DEFAULT '' COMMENT 'ÁßüÊà∑Â≠óÊÆµ',
      c_desc varchar(256) DEFAULT NULL,
      c_use varchar(64) DEFAULT NULL,
      effect varchar(64) DEFAULT NULL,
      type varchar(64) DEFAULT NULL,
      c_schema text,
      encrypted_data_key text NOT NULL COMMENT 'ÁßòÈí•',
      PRIMARY KEY (id),
      UNIQUE KEY uk_configinfo_datagrouptenant (data_id,group_id,tenant_id)
    ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='config_info';
    
    CREATE TABLE IF NOT EXISTS naming_instance_metadata (
      id bigint(20) NOT NULL AUTO_INCREMENT,
      instance_id varchar(255) NOT NULL,
      metadata_key varchar(128) NOT NULL,
      metadata_value varchar(128) NOT NULL,
      gmt_create datetime NOT NULL DEFAULT CURRENT_TIMESTAMP,
      gmt_modified datetime NOT NULL DEFAULT CURRENT_TIMESTAMP,
      PRIMARY KEY (id)
    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
    
    FLUSH PRIVILEGES;
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-nacos-pvc
  namespace: offerkiller-system
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: standard
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql-nacos
  namespace: offerkiller-system
  labels:
    app: mysql-nacos
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mysql-nacos
  template:
    metadata:
      labels:
        app: mysql-nacos
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
        ports:
        - containerPort: 3306
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-nacos-secret
              key: mysql-root-password
        - name: MYSQL_DATABASE
          value: nacos
        - name: MYSQL_USER
          value: nacos
        - name: MYSQL_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-nacos-secret
              key: mysql-password
        volumeMounts:
        - name: mysql-data
          mountPath: /var/lib/mysql
        - name: mysql-init
          mountPath: /docker-entrypoint-initdb.d
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          exec:
            command:
            - mysqladmin
            - ping
            - -h
            - localhost
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          exec:
            command:
            - mysqladmin
            - ping
            - -h
            - localhost
          initialDelaySeconds: 10
          periodSeconds: 5
      volumes:
      - name: mysql-data
        persistentVolumeClaim:
          claimName: mysql-nacos-pvc
      - name: mysql-init
        configMap:
          name: mysql-nacos-config
---
apiVersion: v1
kind: Service
metadata:
  name: mysql
  namespace: offerkiller-system
  labels:
    app: mysql-nacos
spec:
  type: ClusterIP
  ports:
  - port: 3306
    targetPort: 3306
    protocol: TCP
  selector:
    app: mysql-nacos
```

#### 4.4.4 Create Service Mesh Configuration (Istio)

**Create file: `E:\OfferKiller\infrastructure\kubernetes\foundational\istio\istio-installation.yaml`**

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: istio-system
  labels:
    istio-injection: disabled
---
# Istio Base Installation will be done via Helm
# This file serves as reference for the configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: istio-config
  namespace: istio-system
data:
  mesh: |
    defaultConfig:
      proxyStatsMatcher:
        inclusionRegexps:
        - ".*circuit_breakers.*"
        - ".*upstream_rq_retry.*"
        - ".*upstream_rq_pending.*"
        - ".*_cx_.*"
      discoveryRefreshDelay: 10s
      proxyMetadata:
        PILOT_ENABLE_WORKLOAD_ENTRY_AUTOREGISTRATION: true
    defaultProviders:
      metrics:
      - prometheus
    extensionProviders:
    - name: prometheus
      prometheus:
        configOverride:
          metric_relabeling_configs:
          - source_labels: [__name__]
            regex: 'istio_.*'
            target_label: __tmp_istio_metric
    trustDomain: cluster.local
  values.yaml: |
    pilot:
      env:
        EXTERNAL_ISTIOD: false
        PILOT_TRACE_SAMPLING: 1.0
        PILOT_ENABLE_ALPHA_GATEWAY_API: true
    global:
      meshID: mesh1
      multiCluster:
        clusterName: cluster1
      network: network1
```

**Create file: `E:\OfferKiller\infrastructure\kubernetes\foundational\istio\gateway.yaml`**

```yaml
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: offerkiller-gateway
  namespace: offerkiller-system
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "*"
  - port:
      number: 443
      name: https
      protocol: HTTPS
    tls:
      mode: SIMPLE
      credentialName: offerkiller-tls-secret
    hosts:
    - "*"
---
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: nacos-gateway
  namespace: offerkiller-system
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 8848
      name: nacos-http
      protocol: HTTP
    hosts:
    - nacos.offerkiller.local
    - localhost
---
apiVersion: v1
kind: Service
metadata:
  name: istio-ingressgateway-nodeport
  namespace: istio-system
  labels:
    app: istio-ingressgateway
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: 8080
    protocol: TCP
    name: http2
    nodePort: 30080
  - port: 443
    targetPort: 8443
    protocol: TCP
    name: https
    nodePort: 30443
  - port: 8848
    targetPort: 8848
    protocol: TCP
    name: nacos
    nodePort: 30848
  selector:
    app: istio-ingressgateway
```

**Create file: `E:\OfferKiller\infrastructure\kubernetes\foundational\istio\virtual-services.yaml`**

```yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: nacos-vs
  namespace: offerkiller-system
spec:
  hosts:
  - nacos.offerkiller.local
  - localhost
  gateways:
  - nacos-gateway
  http:
  - match:
    - uri:
        prefix: /nacos
    route:
    - destination:
        host: nacos.offerkiller-system.svc.cluster.local
        port:
          number: 8848
    timeout: 30s
    retries:
      attempts: 3
      perTryTimeout: 10s
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: offerkiller-api-vs
  namespace: offerkiller-app
spec:
  hosts:
  - api.offerkiller.local
  - localhost
  gateways:
  - offerkiller-system/offerkiller-gateway
  http:
  - match:
    - uri:
        prefix: /api/user
    route:
    - destination:
        host: user-service.offerkiller-app.svc.cluster.local
        port:
          number: 8081
    timeout: 30s
    retries:
      attempts: 3
      perTryTimeout: 10s
  - match:
    - uri:
        prefix: /api/job
    route:
    - destination:
        host: job-service.offerkiller-app.svc.cluster.local
        port:
          number: 8082
    timeout: 30s
  - match:
    - uri:
        prefix: /api/ai
    route:
    - destination:
        host: ai-orchestration-service.offerkiller-app.svc.cluster.local
        port:
          number: 8083
    timeout: 60s
```

**Create file: `E:\OfferKiller\infrastructure\kubernetes\foundational\istio\destination-rules.yaml`**

```yaml
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: nacos-dr
  namespace: offerkiller-system
spec:
  host: nacos.offerkiller-system.svc.cluster.local
  trafficPolicy:
    loadBalancer:
      simple: LEAST_CONN
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 50
        maxRequestsPerConnection: 2
    circuitBreaker:
      consecutiveErrors: 5
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
    outlierDetection:
      consecutiveErrors: 5
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: user-service-dr
  namespace: offerkiller-app
spec:
  host: user-service.offerkiller-app.svc.cluster.local
  trafficPolicy:
    loadBalancer:
      simple: ROUND_ROBIN
    connectionPool:
      tcp:
        maxConnections: 200
      http:
        http1MaxPendingRequests: 100
        maxRequestsPerConnection: 10
    circuitBreaker:
      consecutiveErrors: 3
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
```

#### 4.4.5 Create Monitoring Stack Configuration

**Create file: `E:\OfferKiller\infrastructure\kubernetes\foundational\monitoring\prometheus\values.yaml`**

```yaml
# Prometheus Helm Chart Values
prometheus:
  prometheusSpec:
    retention: 15d
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: standard
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 50Gi
    
    additionalScrapeConfigs: |
      - job_name: 'nacos'
        static_configs:
          - targets: ['nacos.offerkiller-system.svc.cluster.local:8848']
        scrape_interval: 30s
        metrics_path: '/nacos/actuator/prometheus'
      
      - job_name: 'spring-boot-apps'
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names:
                - offerkiller-app
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
      
      - job_name: 'istio-mesh'
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names:
                - istio-system
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
            action: keep
            regex: istio-proxy;http-monitoring
      
      - job_name: 'istio-policy'
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names:
                - istio-system
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
            action: keep
            regex: istio-policy;http-monitoring
      
      - job_name: 'istio-telemetry'
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names:
                - istio-system
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
            action: keep
            regex: istio-telemetry;http-monitoring

grafana:
  enabled: true
  adminPassword: admin123
  persistence:
    enabled: true
    storageClassName: standard
    size: 10Gi
  
  sidecar:
    dashboards:
      enabled: true
      searchNamespace: ALL
  
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
      - name: 'istio'
        orgId: 1
        folder: 'Istio'
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /var/lib/grafana/dashboards/istio
      - name: 'offerkiller'
        orgId: 1
        folder: 'OfferKiller'
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /var/lib/grafana/dashboards/offerkiller

alertmanager:
  enabled: true
  alertmanagerSpec:
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: standard
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 5Gi

nodeExporter:
  enabled: true

kubeStateMetrics:
  enabled: true

defaultRules:
  create: true
  rules:
    alertmanager: true
    etcd: true
    general: true
    k8s: true
    kubeApiserver: true
    kubePrometheusNodeAlerting: true
    kubePrometheusNodeRecording: true
    kubernetesAbsent: true
    kubernetesApps: true
    kubernetesResources: true
    kubernetesStorage: true
    kubernetesSystem: true
    node: true
    prometheus: true
    prometheusOperator: true
```

#### 4.4.6 Create Security Configuration

**Create file: `E:\OfferKiller\infrastructure\kubernetes\foundational\security\rbac.yaml`**

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: offerkiller-serviceaccount
  namespace: offerkiller-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: offerkiller-clusterrole
rules:
- apiGroups: [""]
  resources: ["pods", "services", "endpoints", "configmaps", "secrets"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["networking.k8s.io"]
  resources: ["ingresses", "networkpolicies"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["networking.istio.io"]
  resources: ["virtualservices", "destinationrules", "gateways"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["metrics.k8s.io"]
  resources: ["pods", "nodes"]
  verbs: ["get", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: offerkiller-clusterrolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: offerkiller-clusterrole
subjects:
- kind: ServiceAccount
  name: offerkiller-serviceaccount
  namespace: offerkiller-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: offerkiller-app
  name: offerkiller-app-role
rules:
- apiGroups: [""]
  resources: ["pods", "services", "endpoints", "configmaps", "secrets"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: offerkiller-app-rolebinding
  namespace: offerkiller-app
subjects:
- kind: ServiceAccount
  name: offerkiller-serviceaccount
  namespace: offerkiller-system
roleRef:
  kind: Role
  name: offerkiller-app-role
  apiGroup: rbac.authorization.k8s.io
```

**Create file: `E:\OfferKiller\infrastructure\kubernetes\foundational\security\network-policies.yaml`**

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: nacos-network-policy
  namespace: offerkiller-system
spec:
  podSelector:
    matchLabels:
      app: nacos
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: offerkiller-app
    - namespaceSelector:
        matchLabels:
          name: istio-system
    - namespaceSelector:
        matchLabels:
          name: offerkiller-monitoring
    ports:
    - protocol: TCP
      port: 8848
    - protocol: TCP
      port: 9848
    - protocol: TCP
      port: 9849
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: mysql-nacos
    ports:
    - protocol: TCP
      port: 3306
  - to: []
    ports:
    - protocol: TCP
      port: 53
    - protocol: UDP
      port: 53
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: mysql-nacos-network-policy
  namespace: offerkiller-system
spec:
  podSelector:
    matchLabels:
      app: mysql-nacos
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: nacos
    ports:
    - protocol: TCP
      port: 3306
  egress:
  - to: []
    ports:
    - protocol: TCP
      port: 53
    - protocol: UDP
      port: 53
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: app-services-network-policy
  namespace: offerkiller-app
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: istio-system
  - from:
    - namespaceSelector:
        matchLabels:
          name: offerkiller-app
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: offerkiller-system
  - to:
    - namespaceSelector:
        matchLabels:
          name: offerkiller-app
  - to: []
    ports:
    - protocol: TCP
      port: 53
    - protocol: UDP
      port: 53
    - protocol: TCP
      port: 80
    - protocol: TCP
      port: 443
```

**Create file: `E:\OfferKiller\infrastructure\kubernetes\foundational\security\pod-security.yaml`**

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: offerkiller-system
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
---
apiVersion: v1
kind: Namespace
metadata:
  name: offerkiller-app
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
---
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: offerkiller-psp
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
    - ALL
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: 'MustRunAsNonRoot'
  supplementalGroups:
    rule: 'MustRunAs'
    ranges:
      - min: 1
        max: 65535
  fsGroup:
    rule: 'MustRunAs'
    ranges:
      - min: 1
        max: 65535
  readOnlyRootFilesystem: false
```

### 4.5 Create Deployment Scripts

#### 4.5.1 Create Master Deployment Script

**Create file: `E:\OfferKiller\scripts\deploy-k8s-foundation.ps1`**

```powershell
# OfferKiller Kubernetes Foundation Deployment Script
param(
    [Parameter(Mandatory=$false)]
    [string]$KubeConfig = "",
    
    [Parameter(Mandatory=$false)]
    [switch]$DryRun = $false,
    
    [Parameter(Mandatory=$false)]
    [switch]$SkipIstio = $false,
    
    [Parameter(Mandatory=$false)]
    [switch]$SkipMonitoring = $false
)

Write-Host "üöÄ OfferKiller Kubernetes Foundation Deployment" -ForegroundColor Green
Write-Host "=================================================" -ForegroundColor Green

# Set kubeconfig if provided
if ($KubeConfig -ne "") {
    $env:KUBECONFIG = $KubeConfig
    Write-Host "üìÅ Using kubeconfig: $KubeConfig" -ForegroundColor Yellow
}

# Function to check if kubectl is available
function Test-Kubectl {
    try {
        kubectl version --client --short
        return $true
    }
    catch {
        Write-Host "‚ùå kubectl is not available. Please install kubectl first." -ForegroundColor Red
        return $false
    }
}

# Function to check if helm is available
function Test-Helm {
    try {
        helm version --short
        return $true
    }
    catch {
        Write-Host "‚ùå Helm is not available. Please install Helm first." -ForegroundColor Red
        return $false
    }
}

# Function to apply Kubernetes manifests
function Deploy-Manifests {
    param([string]$Path, [string]$Description)
    
    Write-Host "üì¶ Deploying $Description..." -ForegroundColor Cyan
    
    if ($DryRun) {
        kubectl apply -f $Path --dry-run=client
    } else {
        kubectl apply -f $Path
    }
    
    if ($LASTEXITCODE -eq 0) {
        Write-Host "‚úÖ $Description deployed successfully" -ForegroundColor Green
    } else {
        Write-Host "‚ùå Failed to deploy $Description" -ForegroundColor Red
        exit 1
    }
}

# Function to wait for deployment
function Wait-ForDeployment {
    param([string]$Namespace, [string]$DeploymentName, [int]$TimeoutSeconds = 300)
    
    Write-Host "‚è≥ Waiting for $DeploymentName in $Namespace to be ready..." -ForegroundColor Yellow
    
    $timeout = [datetime]::Now.AddSeconds($TimeoutSeconds)
    while ([datetime]::Now -lt $timeout) {
        $ready = kubectl get deployment $DeploymentName -n $Namespace -o jsonpath='{.status.readyReplicas}' 2>$null
        $desired = kubectl get deployment $DeploymentName -n $Namespace -o jsonpath='{.spec.replicas}' 2>$null
        
        if ($ready -eq $desired -and $ready -gt 0) {
            Write-Host "‚úÖ $DeploymentName is ready!" -ForegroundColor Green
            return
        }
        
        Start-Sleep -Seconds 10
    }
    
    Write-Host "‚ö†Ô∏è Timeout waiting for $DeploymentName to be ready" -ForegroundColor Yellow
}

# Check prerequisites
if (-not (Test-Kubectl)) { exit 1 }
if (-not (Test-Helm)) { exit 1 }

# Test cluster connectivity
Write-Host "üîç Testing cluster connectivity..." -ForegroundColor Cyan
try {
    kubectl cluster-info
    Write-Host "‚úÖ Cluster is accessible" -ForegroundColor Green
} catch {
    Write-Host "‚ùå Cannot connect to cluster. Please check your kubeconfig." -ForegroundColor Red
    exit 1
}

# Navigate to project root
$ProjectRoot = Split-Path -Parent $PSScriptRoot
Set-Location $ProjectRoot

Write-Host "üìÇ Working directory: $PWD" -ForegroundColor Yellow

# 1. Deploy Namespaces
Deploy-Manifests -Path "infrastructure/kubernetes/foundational/namespace.yaml" -Description "Namespaces"

# 2. Deploy Security Configuration
Deploy-Manifests -Path "infrastructure/kubernetes/foundational/security/" -Description "Security Policies"

# 3. Deploy MySQL for Nacos
Deploy-Manifests -Path "infrastructure/kubernetes/foundational/nacos/mysql-nacos.yaml" -Description "MySQL for Nacos"
Wait-ForDeployment -Namespace "offerkiller-system" -DeploymentName "mysql-nacos"

# 4. Deploy Nacos
Deploy-Manifests -Path "infrastructure/kubernetes/foundational/nacos/" -Description "Nacos Service Registry"
Wait-ForDeployment -Namespace "offerkiller-system" -DeploymentName "nacos"

# 5. Deploy Istio (if not skipped)
if (-not $SkipIstio) {
    Write-Host "üï∏Ô∏è Deploying Istio Service Mesh..." -ForegroundColor Cyan
    
    # Install Istio base
    helm repo add istio https://istio-release.storage.googleapis.com/charts
    helm repo update
    
    if ($DryRun) {
        helm install istio-base istio/base -n istio-system --create-namespace --dry-run
    } else {
        helm install istio-base istio/base -n istio-system --create-namespace --wait
    }
    
    # Install Istiod
    if (-not $DryRun) {
        helm install istiod istio/istiod -n istio-system --wait
    }
    
    # Install Istio Ingress Gateway
    if (-not $DryRun) {
        helm install istio-ingressgateway istio/gateway -n istio-system --wait
    }
    
    # Deploy Istio configurations
    Start-Sleep -Seconds 30
    Deploy-Manifests -Path "infrastructure/kubernetes/foundational/istio/" -Description "Istio Configuration"
    
    Write-Host "‚úÖ Istio Service Mesh deployed" -ForegroundColor Green
}

# 6. Deploy Monitoring Stack (if not skipped)
if (-not $SkipMonitoring) {
    Write-Host "üìä Deploying Monitoring Stack..." -ForegroundColor Cyan
    
    # Add Helm repositories
    helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
    helm repo add grafana https://grafana.github.io/helm-charts
    helm repo add jaegertracing https://jaegertracing.github.io/helm-charts
    helm repo update
    
    # Install Prometheus Operator
    if ($DryRun) {
        helm install prometheus-stack prometheus-community/kube-prometheus-stack `
            -n offerkiller-monitoring --create-namespace `
            -f infrastructure/kubernetes/foundational/monitoring/prometheus/values.yaml `
            --dry-run
    } else {
        helm install prometheus-stack prometheus-community/kube-prometheus-stack `
            -n offerkiller-monitoring --create-namespace `
            -f infrastructure/kubernetes/foundational/monitoring/prometheus/values.yaml `
            --wait --timeout 10m
    }
    
    # Install Jaeger
    if (-not $DryRun) {
        helm install jaeger jaegertracing/jaeger -n offerkiller-monitoring --wait
    }
    
    Write-Host "‚úÖ Monitoring Stack deployed" -ForegroundColor Green
}

# 7. Verify Deployment
Write-Host "üîç Verifying deployment..." -ForegroundColor Cyan

Write-Host "`nüìã Deployment Status:" -ForegroundColor Yellow
kubectl get pods -n offerkiller-system
kubectl get pods -n istio-system
kubectl get pods -n offerkiller-monitoring

Write-Host "`nüåê Service Status:" -ForegroundColor Yellow
kubectl get svc -n offerkiller-system
kubectl get svc -n istio-system

Write-Host "`nüéâ Foundation deployment completed!" -ForegroundColor Green
Write-Host "`nüìù Access Information:" -ForegroundColor Yellow
Write-Host "   Nacos Console:     http://$(minikube ip):30848/nacos (nacos/nacos)" -ForegroundColor Cyan
Write-Host "   Grafana:           http://$(minikube ip):30300 (admin/admin123)" -ForegroundColor Cyan
Write-Host "   Prometheus:        http://$(minikube ip):30900" -ForegroundColor Cyan
Write-Host "   Jaeger:            http://$(minikube ip):30686" -ForegroundColor Cyan

Write-Host "`nüîß Next Steps:" -ForegroundColor Yellow
Write-Host "   1. Verify all services are running" -ForegroundColor White
Write-Host "   2. Configure applications to use Nacos for service discovery" -ForegroundColor White
Write-Host "   3. Deploy application services" -ForegroundColor White
Write-Host "   4. Configure monitoring dashboards" -ForegroundColor White
```

#### 4.5.2 Create Linux Deployment Script

**Create file: `E:\OfferKiller\scripts\deploy-k8s-foundation.sh`**

```bash
#!/bin/bash

# OfferKiller Kubernetes Foundation Deployment Script for Linux

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Configuration
DRY_RUN=false
SKIP_ISTIO=false
SKIP_MONITORING=false
KUBECONFIG_PATH=""
TIMEOUT=300

# Function to print colored output
print_status() {
    echo -e "${GREEN}$1${NC}"
}

print_warning() {
    echo -e "${YELLOW}$1${NC}"
}

print_error() {
    echo -e "${RED}$1${NC}"
}

print_info() {
    echo -e "${CYAN}$1${NC}"
}

# Function to show help
show_help() {
    cat << EOF
OfferKiller Kubernetes Foundation Deployment Script

Usage: $0 [OPTIONS]

Options:
    -d, --dry-run          Perform a dry run without actually deploying
    -k, --kubeconfig PATH  Path to kubeconfig file
    -s, --skip-istio       Skip Istio service mesh deployment
    -m, --skip-monitoring  Skip monitoring stack deployment
    -t, --timeout SECONDS  Timeout for waiting operations (default: 300)
    -h, --help             Show this help message

Examples:
    $0                                    # Deploy everything
    $0 --dry-run                         # Dry run mode
    $0 --skip-istio --skip-monitoring    # Deploy only core services
    $0 --kubeconfig ~/.kube/config       # Use specific kubeconfig

EOF
}

# Parse command line arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        -d|--dry-run)
            DRY_RUN=true
            shift
            ;;
        -k|--kubeconfig)
            KUBECONFIG_PATH="$2"
            shift 2
            ;;
        -s|--skip-istio)
            SKIP_ISTIO=true
            shift
            ;;
        -m|--skip-monitoring)
            SKIP_MONITORING=true
            shift
            ;;
        -t|--timeout)
            TIMEOUT="$2"
            shift 2
            ;;
        -h|--help)
            show_help
            exit 0
            ;;
        *)
            print_error "Unknown option $1"
            show_help
            exit 1
            ;;
    esac
done

# Set kubeconfig if provided
if [[ -n "$KUBECONFIG_PATH" ]]; then
    export KUBECONFIG="$KUBECONFIG_PATH"
    print_warning "Using kubeconfig: $KUBECONFIG_PATH"
fi

print_status "üöÄ OfferKiller Kubernetes Foundation Deployment"
print_status "================================================="

# Function to check if command exists
command_exists() {
    command -v "$1" >/dev/null 2>&1
}

# Check prerequisites
check_prerequisites() {
    print_info "üîç Checking prerequisites..."
    
    if ! command_exists kubectl; then
        print_error "‚ùå kubectl is not installed. Please install kubectl first."
        exit 1
    fi
    
    if ! command_exists helm; then
        print_error "‚ùå Helm is not installed. Please install Helm first."
        exit 1
    fi
    
    # Test cluster connectivity
    if ! kubectl cluster-info >/dev/null 2>&1; then
        print_error "‚ùå Cannot connect to Kubernetes cluster. Please check your kubeconfig."
        exit 1
    fi
    
    print_status "‚úÖ Prerequisites check passed"
}

# Function to apply Kubernetes manifests
deploy_manifests() {
    local path="$1"
    local description="$2"
    
    print_info "üì¶ Deploying $description..."
    
    if [[ "$DRY_RUN" == "true" ]]; then
        kubectl apply -f "$path" --dry-run=client
    else
        kubectl apply -f "$path"
    fi
    
    if [[ $? -eq 0 ]]; then
        print_status "‚úÖ $description deployed successfully"
    else
        print_error "‚ùå Failed to deploy $description"
        exit 1
    fi
}

# Function to wait for deployment
wait_for_deployment() {
    local namespace="$1"
    local deployment_name="$2"
    local timeout_seconds="${3:-$TIMEOUT}"
    
    print_warning "‚è≥ Waiting for $deployment_name in $namespace to be ready..."
    
    if [[ "$DRY_RUN" == "true" ]]; then
        print_info "üèÉ Skipping wait in dry run mode"
        return 0
    fi
    
    local end_time=$((SECONDS + timeout_seconds))
    
    while [[ $SECONDS -lt $end_time ]]; do
        local ready=$(kubectl get deployment "$deployment_name" -n "$namespace" -o jsonpath='{.status.readyReplicas}' 2>/dev/null || echo "0")
        local desired=$(kubectl get deployment "$deployment_name" -n "$namespace" -o jsonpath='{.spec.replicas}' 2>/dev/null || echo "1")
        
        if [[ "$ready" == "$desired" && "$ready" -gt 0 ]]; then
            print_status "‚úÖ $deployment_name is ready!"
            return 0
        fi
        
        sleep 10
    done
    
    print_warning "‚ö†Ô∏è Timeout waiting for $deployment_name to be ready"
    return 1
}

# Function to deploy Helm chart
deploy_helm_chart() {
    local release_name="$1"
    local chart="$2"
    local namespace="$3"
    local values_file="$4"
    local description="$5"
    
    print_info "üì¶ Deploying $description via Helm..."
    
    local helm_cmd="helm install $release_name $chart -n $namespace --create-namespace"
    
    if [[ -n "$values_file" && -f "$values_file" ]]; then
        helm_cmd="$helm_cmd -f $values_file"
    fi
    
    if [[ "$DRY_RUN" == "true" ]]; then
        helm_cmd="$helm_cmd --dry-run"
    else
        helm_cmd="$helm_cmd --wait --timeout 10m"
    fi
    
    eval "$helm_cmd"
    
    if [[ $? -eq 0 ]]; then
        print_status "‚úÖ $description deployed successfully"
    else
        print_error "‚ùå Failed to deploy $description"
        exit 1
    fi
}

# Main deployment function
main() {
    check_prerequisites
    
    # Navigate to project root
    local script_dir="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
    local project_root="$(dirname "$script_dir")"
    cd "$project_root"
    
    print_warning "üìÇ Working directory: $(pwd)"
    
    # 1. Deploy Namespaces
    deploy_manifests "infrastructure/kubernetes/foundational/namespace.yaml" "Namespaces"
    
    # 2. Deploy Security Configuration
    deploy_manifests "infrastructure/kubernetes/foundational/security/" "Security Policies"
    
    # 3. Deploy MySQL for Nacos
    deploy_manifests "infrastructure/kubernetes/foundational/nacos/mysql-nacos.yaml" "MySQL for Nacos"
    wait_for_deployment "offerkiller-system" "mysql-nacos"
    
    # 4. Deploy Nacos
    deploy_manifests "infrastructure/kubernetes/foundational/nacos/" "Nacos Service Registry"
    wait_for_deployment "offerkiller-system" "nacos"
    
    # 5. Deploy Istio (if not skipped)
    if [[ "$SKIP_ISTIO" != "true" ]]; then
        print_info "üï∏Ô∏è Deploying Istio Service Mesh..."
        
        # Add Istio Helm repository
        helm repo add istio https://istio-release.storage.googleapis.com/charts
        helm repo update
        
        # Install Istio components
        deploy_helm_chart "istio-base" "istio/base" "istio-system" "" "Istio Base"
        deploy_helm_chart "istiod" "istio/istiod" "istio-system" "" "Istio Control Plane"
        deploy_helm_chart "istio-ingressgateway" "istio/gateway" "istio-system" "" "Istio Ingress Gateway"
        
        # Wait for Istio to be ready
        if [[ "$DRY_RUN" != "true" ]]; then
            sleep 30
        fi
        
        # Deploy Istio configurations
        deploy_manifests "infrastructure/kubernetes/foundational/istio/" "Istio Configuration"
        
        print_status "‚úÖ Istio Service Mesh deployed"
    fi
    
    # 6. Deploy Monitoring Stack (if not skipped)
    if [[ "$SKIP_MONITORING" != "true" ]]; then
        print_info "üìä Deploying Monitoring Stack..."
        
        # Add Helm repositories
        helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
        helm repo add grafana https://grafana.github.io/helm-charts
        helm repo add jaegertracing https://jaegertracing.github.io/helm-charts
        helm repo update
        
        # Install Prometheus Operator
        deploy_helm_chart "prometheus-stack" "prometheus-community/kube-prometheus-stack" \
            "offerkiller-monitoring" \
            "infrastructure/kubernetes/foundational/monitoring/prometheus/values.yaml" \
            "Prometheus Stack"
        
        # Install Jaeger
        deploy_helm_chart "jaeger" "jaegertracing/jaeger" "offerkiller-monitoring" "" "Jaeger Tracing"
        
        print_status "‚úÖ Monitoring Stack deployed"
    fi
    
    # 7. Verify Deployment
    print_info "üîç Verifying deployment..."
    
    if [[ "$DRY_RUN" != "true" ]]; then
        echo
        print_warning "üìã Deployment Status:"
        kubectl get pods -n offerkiller-system
        kubectl get pods -n istio-system 2>/dev/null || true
        kubectl get pods -n offerkiller-monitoring 2>/dev/null || true
        
        echo
        print_warning "üåê Service Status:"
        kubectl get svc -n offerkiller-system
        kubectl get svc -n istio-system 2>/dev/null || true
        
        echo
        print_status "üéâ Foundation deployment completed!"
        
        # Get minikube IP for access information
        local minikube_ip=""
        if command_exists minikube; then
            minikube_ip=$(minikube ip 2>/dev/null || echo "localhost")
        else
            minikube_ip="localhost"
        fi
        
        echo
        print_warning "üìù Access Information:"
        print_info "   Nacos Console:     http://$minikube_ip:30848/nacos (nacos/nacos)"
        print_info "   Grafana:           http://$minikube_ip:30300 (admin/admin123)"
        print_info "   Prometheus:        http://$minikube_ip:30900"
        print_info "   Jaeger:            http://$minikube_ip:30686"
        
        echo
        print_warning "üîß Next Steps:"
        echo "   1. Verify all services are running"
        echo "   2. Configure applications to use Nacos for service discovery"
        echo "   3. Deploy application services"
        echo "   4. Configure monitoring dashboards"
    else
        print_status "‚úÖ Dry run completed successfully"
    fi
}

# Run main function
main "$@"
```

### 4.6 Linux VM: Deploy the Foundation Services

Now let's execute the deployment on your Linux VM step by step.

#### 4.6.1 Copy Files to Linux VM

**On Windows, push the changes to GitHub first:**

```powershell
# Navigate to project root
cd E:\OfferKiller

# Add all new files
git add .

# Commit changes
git commit -m "Add Kubernetes foundational platform services configuration

- Complete Nacos service registry setup with MySQL backend
- Istio service mesh configuration with gateways and routing
- Monitoring stack with Prometheus, Grafana, and Jaeger
- Security policies with RBAC and network policies
- Comprehensive deployment scripts for both Windows and Linux
- Step-by-step validation and troubleshooting guides"

# Push to GitHub
git push origin main
```

#### 4.6.2 Update Linux VM with Latest Code

**SSH into your Linux VM:**

```bash
# Navigate to project directory
cd ~/offerkiller

# Pull latest changes
git pull origin main

# Make deployment script executable
chmod +x scripts/deploy-k8s-foundation.sh

# Verify files are present
ls -la infrastructure/kubernetes/foundational/
```

#### 4.6.3 Deploy Foundation Services

```bash
# Start with a dry run to verify everything
./scripts/deploy-k8s-foundation.sh --dry-run

# If dry run succeeds, perform actual deployment
./scripts/deploy-k8s-foundation.sh

# Monitor the deployment progress
watch kubectl get pods -A
```

### 4.7 Validation and Testing

#### 4.7.1 Verify Core Services

```bash
# Check all namespaces
kubectl get namespaces

# Check system pods
kubectl get pods -n offerkiller-system

# Check service status
kubectl get svc -n offerkiller-system

# Check Istio components
kubectl get pods -n istio-system

# Check monitoring stack
kubectl get pods -n offerkiller-monitoring
```

#### 4.7.2 Test Nacos Service Registry

```bash
# Get Nacos service URL
minikube service nacos-nodeport -n offerkiller-system --url

# Or access via NodePort
echo "Nacos Console: http://$(minikube ip):30848/nacos"

# Test Nacos API
curl -X GET "http://$(minikube ip):30848/nacos/v1/ns/catalog/services"
```

**You should be able to access Nacos console in your browser:**
- URL: `http://[your-vm-ip]:30848/nacos`
- Username: `nacos`
- Password: `nacos`

#### 4.7.3 Test Service Mesh Connectivity

```bash
# Check Istio injection
kubectl get namespace -L istio-injection

# Check gateways
kubectl get gateway -A

# Check virtual services
kubectl get virtualservice -A

# Test ingress gateway
kubectl get svc istio-ingressgateway -n istio-system
```

#### 4.7.4 Test Monitoring Stack

```bash
# Check Prometheus
kubectl get svc prometheus-stack-kube-prom-prometheus -n offerkiller-monitoring

# Check Grafana
kubectl get svc prometheus-stack-grafana -n offerkiller-monitoring

# Port forward Grafana (in a separate terminal)
kubectl port-forward svc/prometheus-stack-grafana -n offerkiller-monitoring 3000:80

# Access Grafana at http://localhost:3000
# Username: admin, Password: admin123
```

### 4.8 Windows: Configure kubectl to Access the Cluster

**On your Windows PC, configure access to the Linux VM cluster:**

#### 4.8.1 Copy Kubeconfig from Linux VM

**On Linux VM:**

```bash
# Copy kubeconfig content
cat ~/.kube/config
```

**On Windows:**

```powershell
# Create .kube directory
New-Item -ItemType Directory -Force -Path "$env:USERPROFILE\.kube"

# Create config file (replace content with the output from Linux VM)
notepad "$env:USERPROFILE\.kube\config"
```

**Edit the kubeconfig file to use your Linux VM IP instead of localhost:**

```yaml
# Replace server: https://127.0.0.1:8443
# With: server: https://[YOUR_LINUX_VM_IP]:8443
```

#### 4.8.2 Test Connection from Windows

```powershell
# Test kubectl connection
kubectl cluster-info

# Test Helm connection
helm list -A

# Check services
kubectl get svc -A
```

### 4.9 Create Configuration Management Templates

#### 4.9.1 Create Application Configuration Templates

**Create file: `E:\OfferKiller\infrastructure\kubernetes\foundational\config-management\spring-boot-config.yaml`**

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: spring-boot-common-config
  namespace: offerkiller-app
data:
  application.yml: |
    spring:
      application:
        name: ${SPRING_APPLICATION_NAME:unknown-service}
      profiles:
        active: ${SPRING_PROFILES_ACTIVE:dev}
      cloud:
        nacos:
          discovery:
            server-addr: nacos.offerkiller-system.svc.cluster.local:8848
            namespace: ${NACOS_NAMESPACE:public}
            group: ${NACOS_GROUP:DEFAULT_GROUP}
            metadata:
              version: ${SERVICE_VERSION:1.0.0}
              zone: ${DEPLOYMENT_ZONE:default}
          config:
            server-addr: nacos.offerkiller-system.svc.cluster.local:8848
            file-extension: yml
            namespace: ${NACOS_NAMESPACE:public}
            group: ${NACOS_GROUP:DEFAULT_GROUP}
            shared-configs:
              - data-id: common-config.yml
                group: DEFAULT_GROUP
                refresh: true
      datasource:
        driver-class-name: com.mysql.cj.jdbc.Driver
        url: jdbc:mysql://${MYSQL_HOST:mysql}:${MYSQL_PORT:3306}/${MYSQL_DATABASE:offerkiller}?useUnicode=true&characterEncoding=UTF-8&serverTimezone=Asia/Shanghai
        username: ${MYSQL_USERNAME:offerkilleruser}
        password: ${MYSQL_PASSWORD:offerkillerpass123}
        hikari:
          maximum-pool-size: ${DB_POOL_SIZE:20}
          minimum-idle: ${DB_POOL_MIN_IDLE:5}
          idle-timeout: 300000
          max-lifetime: 1800000
      redis:
        host: ${REDIS_HOST:redis}
        port: ${REDIS_PORT:6379}
        password: ${REDIS_PASSWORD:}
        database: ${REDIS_DATABASE:0}
        timeout: 2000ms
        lettuce:
          pool:
            max-active: ${REDIS_POOL_MAX_ACTIVE:10}
            max-idle: ${REDIS_POOL_MAX_IDLE:8}
            min-idle: ${REDIS_POOL_MIN_IDLE:0}
      rabbitmq:
        host: ${RABBITMQ_HOST:rabbitmq}
        port: ${RABBITMQ_PORT:5672}
        username: ${RABBITMQ_USERNAME:offerkilleruser}
        password: ${RABBITMQ_PASSWORD:offerkillerpass123}
        virtual-host: ${RABBITMQ_VHOST:offerkiller}
    
    management:
      endpoints:
        web:
          exposure:
            include: health,info,metrics,prometheus
      endpoint:
        health:
          show-details: always
      metrics:
        export:
          prometheus:
            enabled: true
    
    logging:
      level:
        com.offerkiller: ${LOG_LEVEL:INFO}
        org.springframework.cloud.gateway: ${GATEWAY_LOG_LEVEL:INFO}
      pattern:
        console: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level [%X{traceId},%X{spanId}] %logger{36} - %msg%n"
        file: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level [%X{traceId},%X{spanId}] %logger{36} - %msg%n"
      file:
        name: /app/logs/${spring.application.name}.log
        max-size: 100MB
        max-history: 30
    
    offerkiller:
      ai:
        services:
          base-url: http://python-ai.offerkiller-app.svc.cluster.local:8090
          timeout: 60s
          retry:
            max-attempts: 3
            delay: 1s
      security:
        jwt:
          secret: ${JWT_SECRET:offerkiller-jwt-secret-key-change-in-production}
          expiration: ${JWT_EXPIRATION:86400}
      monitoring:
        tracing:
          enabled: true
          sampling-rate: ${TRACING_SAMPLING_RATE:0.1}
---
apiVersion: v1
kind: Secret
metadata:
  name: spring-boot-secrets
  namespace: offerkiller-app
type: Opaque
data:
  mysql-password: b2ZmZXJraWxsZXJwYXNzMTIz  # offerkillerpass123
  redis-password: ""                          # empty for development
  rabbitmq-password: b2ZmZXJraWxsZXJwYXNzMTIz  # offerkillerpass123
  jwt-secret: b2ZmZXJraWxsZXItand0LXNlY3JldC1rZXktY2hhbmdlLWluLXByb2R1Y3Rpb24=
```

### 4.10 Create Troubleshooting Guide

#### 4.10.1 Common Issues and Solutions

**Create file: `E:\OfferKiller\docs\troubleshooting\k8s-foundation-issues.md`**

```markdown
# Kubernetes Foundation Troubleshooting Guide

## Common Issues and Solutions

### 1. Minikube Startup Issues

**Problem**: Minikube fails to start with insufficient resources
```bash
minikube start --driver=docker --cpus=4 --memory=8192 --disk-size=50gb
```

**Solution**: Increase VM resources or use different driver
```bash
# Stop and delete existing cluster
minikube stop
minikube delete

# Start with more resources
minikube start --driver=docker --cpus=6 --memory=12288 --disk-size=80gb

# Or try different driver
minikube start --driver=virtualbox --cpus=4 --memory=8192
```

### 2. Nacos Connection Issues

**Problem**: Nacos fails to connect to MySQL
```bash
kubectl logs deployment/nacos -n offerkiller-system
```

**Solution**: Check MySQL service and credentials
```bash
# Check MySQL pod status
kubectl get pods -n offerkiller-system | grep mysql

# Check MySQL logs
kubectl logs deployment/mysql-nacos -n offerkiller-system

# Test MySQL connection
kubectl exec -it deployment/mysql-nacos -n offerkiller-system -- mysql -u nacos -p
```

### 3. Istio Installation Issues

**Problem**: Istio components fail to start
```bash
kubectl get pods -n istio-system
```

**Solution**: Reinstall Istio components
```bash
# Remove existing installation
helm uninstall istio-ingressgateway -n istio-system
helm uninstall istiod -n istio-system
helm uninstall istio-base -n istio-system

# Reinstall
helm install istio-base istio/base -n istio-system --create-namespace
helm install istiod istio/istiod -n istio-system --wait
helm install istio-ingressgateway istio/gateway -n istio-system --wait
```

### 4. Service Discovery Issues

**Problem**: Services cannot find each other through Nacos
```bash
# Check Nacos service registry
curl -X GET "http://$(minikube ip):30848/nacos/v1/ns/catalog/services"
```

**Solution**: Verify service registration configuration
```bash
# Check application configuration
kubectl get configmap spring-boot-common-config -n offerkiller-app -o yaml

# Check service logs for registration errors
kubectl logs deployment/user-service -n offerkiller-app
```

### 5. Memory and Resource Issues

**Problem**: Pods are OOMKilled or pending due to insufficient resources
```bash
kubectl top nodes
kubectl top pods -A
```

**Solution**: Adjust resource requests and limits
```bash
# Scale down non-essential services
kubectl scale deployment prometheus-stack-kube-state-metrics -n offerkiller-monitoring --replicas=0

# Increase VM resources or use resource-optimized configurations
```

### 6. Network Policy Issues

**Problem**: Services cannot communicate due to network policies
```bash
kubectl get networkpolicy -A
```

**Solution**: Review and adjust network policies
```bash
# Temporarily disable network policies for testing
kubectl delete networkpolicy --all -n offerkiller-system
kubectl delete networkpolicy --all -n offerkiller-app

# Re-apply with corrected configurations
kubectl apply -f infrastructure/kubernetes/foundational/security/network-policies.yaml
```

## Diagnostic Commands

### General Cluster Health
```bash
# Cluster information
kubectl cluster-info
kubectl get nodes -o wide

# Resource usage
kubectl top nodes
kubectl top pods -A

# Events
kubectl get events --sort-by=.metadata.creationTimestamp -A
```

### Service-Specific Diagnostics
```bash
# Nacos
kubectl describe pod -l app=nacos -n offerkiller-system
kubectl logs -l app=nacos -n offerkiller-system --tail=100

# MySQL
kubectl describe pod -l app=mysql-nacos -n offerkiller-system
kubectl logs -l app=mysql-nacos -n offerkiller-system --tail=100

# Istio
kubectl describe pod -l app=istiod -n istio-system
kubectl logs -l app=istiod -n istio-system --tail=100
```

### Network Troubleshooting
```bash
# Check service endpoints
kubectl get endpoints -A

# Test service connectivity
kubectl run test-pod --image=nicolaka/netshoot --rm -it -- /bin/bash
# Inside the pod:
nslookup nacos.offerkiller-system.svc.cluster.local
curl http://nacos.offerkiller-system.svc.cluster.local:8848/nacos/v1/console/health
```

## Performance Optimization

### Resource Optimization
```bash
# Reduce resource requests for development
kubectl patch deployment nacos -n offerkiller-system -p '{"spec":{"template":{"spec":{"containers":[{"name":"nacos","resources":{"requests":{"memory":"512Mi","cpu":"250m"}}}]}}}}'

# Use horizontal pod autoscaling
kubectl autoscale deployment nacos -n offerkiller-system --cpu-percent=70 --min=1 --max=3
```

### Storage Optimization
```bash
# Check storage usage
kubectl get pvc -A
kubectl describe pvc -A

# Clean up old data if needed
kubectl exec -it deployment/mysql-nacos -n offerkiller-system -- mysql -u root -p -e "SHOW DATABASES;"
```

## Recovery Procedures

### Complete Reset
```bash
# Stop and clean everything
./scripts/deploy-k8s-foundation.sh --help
kubectl delete namespace offerkiller-system offerkiller-app offerkiller-monitoring istio-system --force --grace-period=0

# Wait for cleanup
kubectl get namespaces

# Redeploy
./scripts/deploy-k8s-foundation.sh
```

### Partial Reset (Nacos only)
```bash
# Reset Nacos and MySQL
kubectl delete deployment nacos mysql-nacos -n offerkiller-system
kubectl delete pvc nacos-data-pvc nacos-logs-pvc mysql-nacos-pvc -n offerkiller-system

# Redeploy Nacos components
kubectl apply -f infrastructure/kubernetes/foundational/nacos/
```
```

### 4.11 Final Validation Steps

Now let's validate that everything is working correctly:

```bash
# 1. Check all services are running
kubectl get pods -A

# 2. Verify Nacos is accessible
curl -s http://$(minikube ip):30848/nacos/v1/console/health

# 3. Check Istio mesh status
kubectl get pods -n istio-system

# 4. Verify monitoring stack
kubectl get pods -n offerkiller-monitoring

# 5. Test service discovery registration
# This will be used when we deploy applications in next steps
```

## Success Criteria

Your Step 4 implementation is **successful** when you can confirm:

### ‚úÖ **Core Infrastructure Running**
- [ ] Kubernetes cluster is accessible from both Linux VM and Windows
- [ ] All namespaces created: `offerkiller-system`, `offerkiller-app`, `offerkiller-monitoring`, `istio-system`
- [ ] MySQL for Nacos is running and accessible
- [ ] Nacos service registry is running and accessible via web console

### ‚úÖ **Service Registry Operational**
- [ ] Nacos console accessible at `http://[vm-ip]:30848/nacos`
- [ ] Can login with username `nacos` and password `nacos`
- [ ] Nacos API endpoints responding correctly
- [ ] MySQL backend connected and initialized

### ‚úÖ **Service Mesh Deployed**
- [ ] Istio control plane running in `istio-system` namespace
- [ ] Ingress gateway accessible and configured
- [ ] Virtual services and destination rules applied
- [ ] Sidecar injection enabled for application namespaces

### ‚úÖ **Monitoring Stack Active**
- [ ] Prometheus collecting metrics
- [ ] Grafana dashboards available (admin/admin123)
- [ ] Jaeger tracing components running
- [ ] All monitoring services accessible via NodePort

### ‚úÖ **Security Policies Applied**
- [ ] RBAC roles and bindings configured
- [ ] Network policies restricting pod-to-pod communication
- [ ] Pod security standards enforced
- [ ] Service accounts with minimal required permissions

### ‚úÖ **Configuration Management Ready**
- [ ] ConfigMaps created for application configuration
- [ ] Secrets properly stored and accessible
- [ ] Environment-specific configurations prepared
- [ ] Nacos configuration center accessible for dynamic config

**Test Command to Verify Success:**
```bash
# Run this comprehensive test
curl -s http://$(minikube ip):30848/nacos/v1/console/health && \
kubectl get pods -A | grep -E "(Running|Ready)" | wc -l && \
echo "‚úÖ Foundation services are ready for application deployment!"
```

## Next Steps

After completing Step 4 successfully:

1. **‚úÖ Step 1**: Docker development environment ‚úì
2. **‚úÖ Step 2**: Git monorepo structure ‚úì  
3. **‚úÖ Step 3**: GitHub Actions CI/CD pipeline ‚úì
4. **‚úÖ Step 4**: Kubernetes foundational services ‚úì
5. **‚û°Ô∏è Step 5**: Helm charts for Redis, RabbitMQ, and vector database

You now have a **complete foundational platform** ready for:
- **Service Discovery**: Applications can register and discover each other via Nacos
- **Configuration Management**: Centralized configuration via Nacos config center
- **Service Mesh**: Traffic management, security, and observability via Istio
- **Monitoring**: Full observability stack with metrics, logs, and tracing
- **Security**: RBAC, network policies, and pod security standards
- **Scalability**: Kubernetes-native scaling and resource management

Your microservices applications can now be deployed and will automatically benefit from service discovery, configuration management, traffic routing, and comprehensive monitoring!

## Troubleshooting

If you encounter any issues during deployment:

1. **Check the troubleshooting guide**: `docs/troubleshooting/k8s-foundation-issues.md`
2. **View pod logs**: `kubectl logs -f deployment/[service-name] -n [namespace]`
3. **Check events**: `kubectl get events --sort-by=.metadata.creationTimestamp -A`
4. **Restart services**: `kubectl rollout restart deployment/[service-name] -n [namespace]`
5. **Use the deployment script with dry-run**: `./scripts/deploy-k8s-foundation.sh --dry-run`

Remember: You can always run `./scripts/deploy-k8s-foundation.sh --help` to see all available options for the deployment script.
