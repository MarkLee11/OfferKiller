# Step 5: Helm Installation of Redis Cluster, RabbitMQ Message Broker, and Vector Database

## Overview
Deploy high-availability Redis cluster, RabbitMQ message broker, and vector database (ChromaDB) using Helm charts with enterprise-grade configurations. This step establishes the core data layer infrastructure for caching, messaging, and AI vector operations that will support all OfferKiller microservices.

## Prerequisites Checklist
Before starting, ensure you have:
- [ ] Step 1 completed (Docker environment running on Linux VM)
- [ ] Step 2 completed (Git monorepo structure initialized on Windows)
- [ ] Step 3 completed (GitHub Actions CI/CD pipeline setup)
- [ ] Step 4 completed (Kubernetes foundational platform services)
- [ ] Linux VM with at least 16GB RAM and 100GB disk space
- [ ] Kubernetes cluster running with foundational services deployed
- [ ] Helm package manager installed (version 3.10+)
- [ ] kubectl CLI tool configured and working
- [ ] Nacos service registry operational
- [ ] Istio service mesh deployed and functional

## What We'll Create
```
infrastructure/helm/
‚îú‚îÄ‚îÄ charts/                           # Custom Helm charts
‚îÇ   ‚îú‚îÄ‚îÄ redis-cluster/               # Redis HA cluster chart
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Chart.yaml               # Chart metadata
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ values.yaml              # Default values
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ templates/               # Kubernetes manifests
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ configmap.yaml       # Redis configuration
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ service.yaml         # Service definitions
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ statefulset.yaml     # Redis nodes
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ sentinel.yaml        # Redis Sentinel
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ pdb.yaml             # Pod disruption budget
‚îÇ   ‚îú‚îÄ‚îÄ rabbitmq-ha/                 # RabbitMQ HA cluster chart
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Chart.yaml               # Chart metadata
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ values.yaml              # Default values
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ templates/               # Kubernetes manifests
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ configmap.yaml       # RabbitMQ configuration
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ service.yaml         # Service definitions
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ statefulset.yaml     # RabbitMQ nodes
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ secret.yaml          # Credentials
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ pdb.yaml             # Pod disruption budget
‚îÇ   ‚îî‚îÄ‚îÄ vector-database/             # ChromaDB HA cluster chart
‚îÇ       ‚îú‚îÄ‚îÄ Chart.yaml               # Chart metadata
‚îÇ       ‚îú‚îÄ‚îÄ values.yaml              # Default values
‚îÇ       ‚îî‚îÄ‚îÄ templates/               # Kubernetes manifests
‚îÇ           ‚îú‚îÄ‚îÄ deployment.yaml      # ChromaDB deployment
‚îÇ           ‚îú‚îÄ‚îÄ service.yaml         # Service definitions
‚îÇ           ‚îú‚îÄ‚îÄ pvc.yaml             # Persistent storage
‚îÇ           ‚îî‚îÄ‚îÄ configmap.yaml       # Configuration
‚îú‚îÄ‚îÄ values/                          # Environment-specific values
‚îÇ   ‚îú‚îÄ‚îÄ development/                 # Development environment
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ redis-cluster.yaml       # Redis dev config
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rabbitmq-ha.yaml         # RabbitMQ dev config
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vector-database.yaml     # Vector DB dev config
‚îÇ   ‚îú‚îÄ‚îÄ staging/                     # Staging environment
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ redis-cluster.yaml       # Redis staging config
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rabbitmq-ha.yaml         # RabbitMQ staging config
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vector-database.yaml     # Vector DB staging config
‚îÇ   ‚îî‚îÄ‚îÄ production/                  # Production environment
‚îÇ       ‚îú‚îÄ‚îÄ redis-cluster.yaml       # Redis prod config
‚îÇ       ‚îú‚îÄ‚îÄ rabbitmq-ha.yaml         # RabbitMQ prod config
‚îÇ       ‚îî‚îÄ‚îÄ vector-database.yaml     # Vector DB prod config
‚îî‚îÄ‚îÄ scripts/                         # Deployment automation
    ‚îú‚îÄ‚îÄ deploy-data-layer.ps1        # Windows deployment script
    ‚îú‚îÄ‚îÄ deploy-data-layer.sh         # Linux deployment script
    ‚îú‚îÄ‚îÄ backup-restore.sh            # Backup and restore procedures
    ‚îî‚îÄ‚îÄ monitoring-setup.sh          # Monitoring configuration
```

## Platform Architecture
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                           Data Layer Infrastructure                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Redis Cluster  ‚îÇ  ‚îÇ RabbitMQ Cluster‚îÇ  ‚îÇ    Vector Database          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                 ‚îÇ  ‚îÇ                 ‚îÇ  ‚îÇ                             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚îÇRedis‚îÇ ‚îÇRedis‚îÇ ‚îÇ  ‚îÇ ‚îÇRMQ-1‚îÇ ‚îÇRMQ-2‚îÇ ‚îÇ  ‚îÇ ‚îÇChromaDB ‚îÇ ‚îÇ Qdrant      ‚îÇ ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚îÇ M/S ‚îÇ ‚îÇ M/S ‚îÇ ‚îÇ  ‚îÇ ‚îÇ     ‚îÇ ‚îÇ     ‚îÇ ‚îÇ  ‚îÇ ‚îÇCollection‚îÇ ‚îÇ Collection  ‚îÇ ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ  ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ  ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚îÇRedis‚îÇ ‚îÇSenti‚îÇ ‚îÇ  ‚îÇ ‚îÇRMQ-3‚îÇ         ‚îÇ  ‚îÇ ‚îÇ Volume  ‚îÇ ‚îÇ Backup      ‚îÇ ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚îÇ M/S ‚îÇ ‚îÇ nel ‚îÇ ‚îÇ  ‚îÇ ‚îÇ     ‚îÇ         ‚îÇ  ‚îÇ ‚îÇ Storage ‚îÇ ‚îÇ Storage     ‚îÇ ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ  ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ  ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ           ‚îÇ                     ‚îÇ                         ‚îÇ                ‚îÇ
‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ                                 ‚îÇ                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ                      Service Integration Layer                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇCache Service‚îÇ  ‚îÇQueue Service‚îÇ  ‚îÇVector Service‚îÇ  ‚îÇMonitoring ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ(Session,    ‚îÇ  ‚îÇ(Events,     ‚îÇ  ‚îÇ(Embeddings, ‚îÇ  ‚îÇ(Metrics,  ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Temp Data)  ‚îÇ  ‚îÇ Async Jobs) ‚îÇ  ‚îÇ Similarity) ‚îÇ  ‚îÇ Alerts)   ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Step-by-Step Implementation

### 5.1 Windows: Create Helm Chart Structures

**Navigate to your project directory and create the Helm chart structures:**

#### 5.1.1 Create Redis Cluster Helm Chart

**Create file: `E:\OfferKiller\infrastructure\helm\charts\redis-cluster\Chart.yaml`**

```yaml
apiVersion: v2
name: redis-cluster
description: High-availability Redis cluster for OfferKiller
type: application
version: 1.0.0
appVersion: "7.2.0"
keywords:
  - redis
  - cache
  - database
  - nosql
home: https://redis.io/
sources:
  - https://github.com/redis/redis
maintainers:
  - name: OfferKiller Team
    email: devops@offerkiller.com
dependencies:
  - name: common
    repository: https://charts.bitnami.com/bitnami
    version: 2.x.x
annotations:
  category: Database
  licenses: BSD-3-Clause
```

**Create file: `E:\OfferKiller\infrastructure\helm\charts\redis-cluster\values.yaml`**

```yaml
# Redis Cluster Configuration
redis:
  image:
    registry: docker.io
    repository: redis
    tag: "7.2.0-alpine"
    pullPolicy: IfNotPresent
  
  # Cluster configuration
  cluster:
    enabled: true
    nodes: 6
    replicas: 1
    
  # Authentication
  auth:
    enabled: true
    password: "redis123change"
    existingSecret: ""
    existingSecretPasswordKey: ""
  
  # Resource configuration
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "512Mi"
      cpu: "500m"
  
  # Persistence
  persistence:
    enabled: true
    storageClass: "standard"
    size: "8Gi"
    accessModes:
      - ReadWriteOnce
  
  # Configuration
  configuration: |
    # Network
    bind 0.0.0.0
    port 6379
    tcp-keepalive 300
    
    # Memory management
    maxmemory 400mb
    maxmemory-policy allkeys-lru
    
    # Persistence
    save 900 1
    save 300 10
    save 60 10000
    
    # Cluster
    cluster-enabled yes
    cluster-config-file nodes.conf
    cluster-node-timeout 5000
    
    # Logging
    loglevel notice
    
    # Security
    protected-mode no

# Redis Sentinel Configuration
sentinel:
  enabled: true
  image:
    registry: docker.io
    repository: redis
    tag: "7.2.0-alpine"
  
  # Sentinel specific configuration
  resources:
    requests:
      memory: "64Mi"
      cpu: "50m"
    limits:
      memory: "128Mi"
      cpu: "100m"
  
  configuration: |
    port 26379
    sentinel monitor redis-master redis-node-0.redis.offerkiller-data.svc.cluster.local 6379 2
    sentinel down-after-milliseconds redis-master 5000
    sentinel parallel-syncs redis-master 1
    sentinel failover-timeout redis-master 10000

# Service configuration
service:
  type: ClusterIP
  port: 6379
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "6379"

# ServiceMonitor for Prometheus
serviceMonitor:
  enabled: true
  labels:
    app: redis-cluster
  interval: 30s
  path: /metrics

# Network Policy
networkPolicy:
  enabled: true
  ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            name: offerkiller-app
      - namespaceSelector:
          matchLabels:
            name: offerkiller-system
      ports:
      - protocol: TCP
        port: 6379
      - protocol: TCP
        port: 26379

# Pod Disruption Budget
podDisruptionBudget:
  enabled: true
  minAvailable: 2

# Horizontal Pod Autoscaler
autoscaling:
  enabled: false
  minReplicas: 3
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

# Security Context
securityContext:
  runAsNonRoot: true
  runAsUser: 999
  fsGroup: 999

# Affinity and tolerations
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchLabels:
            app: redis-cluster
        topologyKey: kubernetes.io/hostname

# Additional labels
labels:
  app: redis-cluster
  version: "7.2.0"
  component: cache

# Backup configuration
backup:
  enabled: true
  schedule: "0 2 * * *"
  retention: "7d"
  storage:
    storageClass: "standard"
    size: "5Gi"
```

#### 5.1.2 Create RabbitMQ Cluster Helm Chart

**Create file: `E:\OfferKiller\infrastructure\helm\charts\rabbitmq-ha\Chart.yaml`**

```yaml
apiVersion: v2
name: rabbitmq-ha
description: High-availability RabbitMQ cluster for OfferKiller message broker
type: application
version: 1.0.0
appVersion: "3.12.0"
keywords:
  - rabbitmq
  - message broker
  - amqp
  - queue
home: https://rabbitmq.com/
sources:
  - https://github.com/rabbitmq/rabbitmq-server
maintainers:
  - name: OfferKiller Team
    email: devops@offerkiller.com
dependencies:
  - name: common
    repository: https://charts.bitnami.com/bitnami
    version: 2.x.x
annotations:
  category: Message Broker
  licenses: MPL-2.0
```

**Create file: `E:\OfferKiller\infrastructure\helm\charts\rabbitmq-ha\values.yaml`**

```yaml
# RabbitMQ HA Configuration
rabbitmq:
  image:
    registry: docker.io
    repository: rabbitmq
    tag: "3.12.0-management-alpine"
    pullPolicy: IfNotPresent
  
  # Cluster configuration
  clustering:
    enabled: true
    rebalance: true
    forceBoot: false
  
  # Authentication
  auth:
    username: "offerkilleruser"
    password: "rabbitmq123change"
    existingPasswordSecret: ""
    existingErlangSecret: ""
    erlangCookie: "offerkiller-rabbitmq-cookie-change-in-production"
  
  # Resource configuration
  resources:
    requests:
      memory: "512Mi"
      cpu: "200m"
    limits:
      memory: "1Gi"
      cpu: "1000m"
  
  # Persistence
  persistence:
    enabled: true
    storageClass: "standard"
    size: "8Gi"
    accessModes:
      - ReadWriteOnce
  
  # Configuration
  configuration: |
    # Clustering
    cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s
    cluster_formation.k8s.host = kubernetes.default.svc.cluster.local
    cluster_formation.k8s.address_type = hostname
    cluster_formation.node_cleanup.interval = 30
    cluster_formation.node_cleanup.only_log_warning = true
    cluster_partition_handling = autoheal
    
    # Queue management
    queue_master_locator = min-masters
    
    # Memory management
    vm_memory_high_watermark.relative = 0.6
    disk_free_limit.relative = 2.0
    
    # Networking
    listeners.tcp.default = 5672
    management.tcp.port = 15672
    
    # SSL/TLS
    listeners.ssl.default = 5671
    ssl_options.cacertfile = /opt/rabbitmq/ssl/ca_certificate.pem
    ssl_options.certfile = /opt/rabbitmq/ssl/server_certificate.pem
    ssl_options.keyfile = /opt/rabbitmq/ssl/server_key.pem
    ssl_options.verify = verify_peer
    ssl_options.fail_if_no_peer_cert = false
    
    # Logging
    log.file.level = info
    log.connection.level = info
    log.channel.level = info
    log.queue.level = info
    log.mirroring.level = info
    log.federation.level = info
    log.upgrade.level = info
    
    # Management plugin
    management.rates_mode = basic
    management.sample_retention_policies.global.minute = 5
    management.sample_retention_policies.global.hour = 60
    management.sample_retention_policies.global.day = 1440

# Load balancer configuration
loadBalancer:
  enabled: true
  port: 5672
  
# Service configuration
service:
  type: ClusterIP
  amqpPort: 5672
  amqpTlsPort: 5671
  managerPort: 15672
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "15692"
    prometheus.io/path: "/metrics"

# Ingress configuration
ingress:
  enabled: true
  ingressClassName: "istio"
  hostname: "rabbitmq.offerkiller.local"
  path: "/"
  tls: true
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /

# ServiceMonitor for Prometheus
serviceMonitor:
  enabled: true
  labels:
    app: rabbitmq-ha
  interval: 30s
  path: /metrics
  port: prometheus

# Plugins configuration
plugins:
  list: |
    rabbitmq_management
    rabbitmq_peer_discovery_k8s
    rabbitmq_prometheus
    rabbitmq_shovel
    rabbitmq_shovel_management
    rabbitmq_federation
    rabbitmq_federation_management

# Policies and definitions
policies:
  - name: "ha-policy"
    pattern: ".*"
    definition:
      ha-mode: "exactly"
      ha-params: 2
      ha-sync-mode: "automatic"
      ha-sync-batch-size: 1
      queue-master-locator: "min-masters"

# Network Policy
networkPolicy:
  enabled: true
  ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            name: offerkiller-app
      - namespaceSelector:
          matchLabels:
            name: offerkiller-system
      ports:
      - protocol: TCP
        port: 5672
      - protocol: TCP
        port: 5671
      - protocol: TCP
        port: 15672

# Pod Disruption Budget
podDisruptionBudget:
  enabled: true
  minAvailable: 2

# Horizontal Pod Autoscaler
autoscaling:
  enabled: false
  minReplicas: 3
  maxReplicas: 5
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

# Security Context
securityContext:
  runAsNonRoot: true
  runAsUser: 999
  fsGroup: 999

# Affinity and tolerations
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchLabels:
            app: rabbitmq-ha
        topologyKey: kubernetes.io/hostname

# Additional labels
labels:
  app: rabbitmq-ha
  version: "3.12.0"
  component: message-broker

# Backup configuration
backup:
  enabled: true
  schedule: "0 3 * * *"
  retention: "7d"
  storage:
    storageClass: "standard"
    size: "5Gi"

# Management UI configuration
management:
  enabled: true
  nodePort: 31672
  
# Monitoring and health checks
livenessProbe:
  enabled: true
  initialDelaySeconds: 60
  periodSeconds: 30
  timeoutSeconds: 10
  failureThreshold: 3

readinessProbe:
  enabled: true
  initialDelaySeconds: 20
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

# Replicas configuration
replicaCount: 3
```

#### 5.1.3 Create Vector Database Helm Chart

**Create file: `E:\OfferKiller\infrastructure\helm\charts\vector-database\Chart.yaml`**

```yaml
apiVersion: v2
name: vector-database
description: High-availability vector database cluster for OfferKiller AI operations
type: application
version: 1.0.0
appVersion: "0.4.0"
keywords:
  - vector
  - database
  - ai
  - ml
  - chroma
  - embeddings
home: https://www.trychroma.com/
sources:
  - https://github.com/chroma-core/chroma
maintainers:
  - name: OfferKiller Team
    email: devops@offerkiller.com
dependencies:
  - name: common
    repository: https://charts.bitnami.com/bitnami
    version: 2.x.x
annotations:
  category: Database
  licenses: Apache-2.0
```

**Create file: `E:\OfferKiller\infrastructure\helm\charts\vector-database\values.yaml`**

```yaml
# Vector Database Configuration
chromadb:
  image:
    registry: docker.io
    repository: chromadb/chroma
    tag: "0.4.15"
    pullPolicy: IfNotPresent
  
  # Authentication and security
  auth:
    enabled: true
    provider: "chromadb.auth.simple.SimpleAuthenticationServerProvider"
    credentials: "admin:vectordb123change"
  
  # Resource configuration
  resources:
    requests:
      memory: "1Gi"
      cpu: "500m"
    limits:
      memory: "4Gi"
      cpu: "2000m"
  
  # Persistence configuration
  persistence:
    enabled: true
    storageClass: "standard"
    size: "50Gi"
    accessModes:
      - ReadWriteOnce
    mountPath: "/chroma/chroma"
  
  # Configuration
  configuration:
    host: "0.0.0.0"
    port: 8000
    cors_allow_origins: ["*"]
    log_level: "INFO"
    anonymized_telemetry: false
    
    # Storage backend
    chroma_db_impl: "chromadb.db.duckdb.DuckDB"
    chroma_server_nofile: 65535
    
    # Performance tuning
    max_batch_size: 5461
    chroma_segment_cache_policy: "LRU"
    chroma_segment_cache_size: 1000
    
    # API configuration
    chroma_server_grpc_port: 50051
    chroma_server_http_port: 8000
    
    # Backup and recovery
    chroma_sysdb_impl: "chromadb.sysdb.impl.sqlite.SqliteDB"
    chroma_producer_impl: "chromadb.ingest.impl.simple.SimpleProducer"
    chroma_consumer_impl: "chromadb.ingest.impl.simple.SimpleConsumer"

# Qdrant alternative configuration (optional)
qdrant:
  enabled: false
  image:
    registry: docker.io
    repository: qdrant/qdrant
    tag: "v1.6.0"
  
  resources:
    requests:
      memory: "512Mi"
      cpu: "250m"
    limits:
      memory: "2Gi"
      cpu: "1000m"
  
  persistence:
    enabled: true
    storageClass: "standard"
    size: "20Gi"
    
  configuration:
    log_level: "INFO"
    storage:
      optimizers:
        deleted_threshold: 0.2
        vacuum_min_vector_number: 1000
        default_segment_number: 0
      wal:
        wal_capacity_mb: 32
        wal_segments_ahead: 0
      performance:
        max_search_threads: 0
        max_optimization_threads: 1

# Service configuration
service:
  type: ClusterIP
  chromaPort: 8000
  chromaGrpcPort: 50051
  qdrantPort: 6333
  qdrantGrpcPort: 6334
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8000"
    prometheus.io/path: "/api/v1/heartbeat"

# Ingress configuration
ingress:
  enabled: true
  ingressClassName: "istio"
  hostname: "vectordb.offerkiller.local"
  path: "/"
  tls: true
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /

# ServiceMonitor for Prometheus
serviceMonitor:
  enabled: true
  labels:
    app: vector-database
  interval: 30s
  path: /api/v1/heartbeat

# Network Policy
networkPolicy:
  enabled: true
  ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            name: offerkiller-app
      - namespaceSelector:
          matchLabels:
            name: offerkiller-system
      ports:
      - protocol: TCP
        port: 8000
      - protocol: TCP
        port: 50051
      - protocol: TCP
        port: 6333
      - protocol: TCP
        port: 6334

# Pod Disruption Budget
podDisruptionBudget:
  enabled: true
  minAvailable: 1

# Horizontal Pod Autoscaler
autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 5
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

# Security Context
securityContext:
  runAsNonRoot: true
  runAsUser: 1000
  fsGroup: 1000

# Affinity and tolerations
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchLabels:
            app: vector-database
        topologyKey: kubernetes.io/hostname

# Additional labels
labels:
  app: vector-database
  version: "0.4.15"
  component: vector-store

# Backup configuration
backup:
  enabled: true
  schedule: "0 4 * * *"
  retention: "14d"
  storage:
    storageClass: "standard"
    size: "10Gi"

# Health checks
livenessProbe:
  enabled: true
  httpGet:
    path: "/api/v1/heartbeat"
    port: 8000
  initialDelaySeconds: 30
  periodSeconds: 30
  timeoutSeconds: 10
  failureThreshold: 3

readinessProbe:
  enabled: true
  httpGet:
    path: "/api/v1/heartbeat"
    port: 8000
  initialDelaySeconds: 10
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

# Init containers for data migration/setup
initContainers:
  - name: chroma-init
    image: busybox:1.35
    command:
      - sh
      - -c
      - |
        echo "Initializing ChromaDB storage..."
        mkdir -p /chroma/chroma
        chown -R 1000:1000 /chroma/chroma
        echo "Initialization complete"
    volumeMounts:
      - name: data
        mountPath: /chroma/chroma

# Collection management
collections:
  default:
    - name: "resumes"
      metadata:
        description: "Resume embeddings for similarity search"
        distance_function: "cosine"
    - name: "job_descriptions"
      metadata:
        description: "Job description embeddings for matching"
        distance_function: "cosine"
    - name: "skills"
      metadata:
        description: "Skill embeddings for gap analysis"
        distance_function: "cosine"
    - name: "interview_questions"
      metadata:
        description: "Interview question embeddings"
        distance_function: "cosine"

# Replicas configuration
replicaCount: 2
```

### 5.2 Windows: Create Environment-Specific Values Files

Now create the environment-specific configuration files:

#### 5.2.1 Development Environment Values

**Create file: `E:\OfferKiller\infrastructure\helm\values\development\redis-cluster.yaml`**

```yaml
# Redis Development Environment Configuration
redis:
  # Reduced resources for development
  resources:
    requests:
      memory: "128Mi"
      cpu: "50m"
    limits:
      memory: "256Mi"
      cpu: "200m"
  
  # Smaller cluster for development
  cluster:
    nodes: 3
    replicas: 1
  
  # Reduced persistence
  persistence:
    size: "2Gi"
  
  # Development-specific configuration
  configuration: |
    bind 0.0.0.0
    port 6379
    maxmemory 200mb
    maxmemory-policy allkeys-lru
    save 300 10
    cluster-enabled yes
    cluster-config-file nodes.conf
    cluster-node-timeout 5000
    loglevel notice
    protected-mode no

# Disable backup in development
backup:
  enabled: false

# Single replica for development
replicaCount: 3

# Development-specific labels
labels:
  environment: "development"
  app: redis-cluster
  version: "7.2.0"
```

**Create file: `E:\OfferKiller\infrastructure\helm\values\development\rabbitmq-ha.yaml`**

```yaml
# RabbitMQ Development Environment Configuration
rabbitmq:
  # Reduced resources for development
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "512Mi"
      cpu: "500m"
  
  # Smaller persistence
  persistence:
    size: "4Gi"
  
  # Simplified configuration for development
  configuration: |
    cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s
    cluster_formation.k8s.host = kubernetes.default.svc.cluster.local
    cluster_formation.k8s.address_type = hostname
    cluster_partition_handling = autoheal
    queue_master_locator = min-masters
    vm_memory_high_watermark.relative = 0.8
    disk_free_limit.relative = 1.0
    listeners.tcp.default = 5672
    management.tcp.port = 15672
    log.file.level = info
    management.rates_mode = basic

# Disable backup in development
backup:
  enabled: false

# Reduced replicas for development
replicaCount: 2

# Development-specific labels
labels:
  environment: "development"
  app: rabbitmq-ha
  version: "3.12.0"

# Simplified policies for development
policies:
  - name: "ha-policy-dev"
    pattern: ".*"
    definition:
      ha-mode: "exactly"
      ha-params: 1
      ha-sync-mode: "automatic"
```

**Create file: `E:\OfferKiller\infrastructure\helm\values\development\vector-database.yaml`**

```yaml
# Vector Database Development Environment Configuration
chromadb:
  # Reduced resources for development
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "1Gi"
      cpu: "500m"
  
  # Smaller persistence
  persistence:
    size: "10Gi"
  
  # Development-specific configuration
  configuration:
    host: "0.0.0.0"
    port: 8000
    cors_allow_origins: ["*"]
    log_level: "DEBUG"
    anonymized_telemetry: false
    max_batch_size: 1000
    chroma_segment_cache_size: 100

# Disable Qdrant in development
qdrant:
  enabled: false

# Disable backup in development
backup:
  enabled: false

# Single replica for development
replicaCount: 1

# Development-specific labels
labels:
  environment: "development"
  app: vector-database
  version: "0.4.15"

# Simplified collections for development
collections:
  default:
    - name: "dev_resumes"
      metadata:
        description: "Development resume embeddings"
        distance_function: "cosine"
    - name: "dev_jobs"
      metadata:
        description: "Development job embeddings"
        distance_function: "cosine"
```

### 5.3 Windows: Create Deployment Scripts

#### 5.3.1 Create Windows PowerShell Deployment Script

**Create file: `E:\OfferKiller\scripts\deploy-data-layer.ps1`**

```powershell
# OfferKiller Data Layer Deployment Script
param(
    [Parameter(Mandatory=$false)]
    [ValidateSet("development", "staging", "production")]
    [string]$Environment = "development",
    
    [Parameter(Mandatory=$false)]
    [string]$KubeConfig = "",
    
    [Parameter(Mandatory=$false)]
    [switch]$DryRun = $false,
    
    [Parameter(Mandatory=$false)]
    [switch]$SkipRedis = $false,
    
    [Parameter(Mandatory=$false)]
    [switch]$SkipRabbitMQ = $false,
    
    [Parameter(Mandatory=$false)]
    [switch]$SkipVectorDB = $false,
    
    [Parameter(Mandatory=$false)]
    [switch]$Force = $false
)

Write-Host "üöÄ OfferKiller Data Layer Deployment ($Environment)" -ForegroundColor Green
Write-Host "========================================================" -ForegroundColor Green

# Set kubeconfig if provided
if ($KubeConfig -ne "") {
    $env:KUBECONFIG = $KubeConfig
    Write-Host "üìÅ Using kubeconfig: $KubeConfig" -ForegroundColor Yellow
}

# Function to check prerequisites
function Test-Prerequisites {
    Write-Host "üîç Checking prerequisites..." -ForegroundColor Cyan
    
    # Check kubectl
    try {
        kubectl version --client --short | Out-Null
        Write-Host "‚úÖ kubectl is available" -ForegroundColor Green
    }
    catch {
        Write-Host "‚ùå kubectl is not available" -ForegroundColor Red
        return $false
    }
    
    # Check helm
    try {
        helm version --short | Out-Null
        Write-Host "‚úÖ Helm is available" -ForegroundColor Green
    }
    catch {
        Write-Host "‚ùå Helm is not available" -ForegroundColor Red
        return $false
    }
    
    # Check cluster connectivity
    try {
        kubectl cluster-info | Out-Null
        Write-Host "‚úÖ Cluster is accessible" -ForegroundColor Green
    }
    catch {
        Write-Host "‚ùå Cannot connect to cluster" -ForegroundColor Red
        return $false
    }
    
    # Check required namespaces
    $namespaces = @("offerkiller-data", "offerkiller-system")
    foreach ($ns in $namespaces) {
        $exists = kubectl get namespace $ns --ignore-not-found=true
        if (-not $exists) {
            Write-Host "üì¶ Creating namespace: $ns" -ForegroundColor Yellow
            if (-not $DryRun) {
                kubectl create namespace $ns
            }
        }
    }
    
    return $true
}

# Function to deploy Helm chart
function Deploy-HelmChart {
    param(
        [string]$ChartPath,
        [string]$ReleaseName,
        [string]$Namespace,
        [string]$ValuesFile,
        [string]$Description
    )
    
    Write-Host "üì¶ Deploying $Description..." -ForegroundColor Cyan
    
    $helmCmd = "helm upgrade --install $ReleaseName $ChartPath -n $Namespace --create-namespace"
    
    if ($ValuesFile -and (Test-Path $ValuesFile)) {
        $helmCmd += " -f $ValuesFile"
    }
    
    if ($DryRun) {
        $helmCmd += " --dry-run"
    } else {
        $helmCmd += " --wait --timeout 10m"
    }
    
    if ($Force) {
        $helmCmd += " --force"
    }
    
    Write-Host "üîß Command: $helmCmd" -ForegroundColor Gray
    
    try {
        Invoke-Expression $helmCmd
        Write-Host "‚úÖ $Description deployed successfully" -ForegroundColor Green
        return $true
    }
    catch {
        Write-Host "‚ùå Failed to deploy $Description" -ForegroundColor Red
        Write-Host $_.Exception.Message -ForegroundColor Red
        return $false
    }
}

# Function to wait for deployment
function Wait-ForDeployment {
    param(
        [string]$Namespace,
        [string]$DeploymentName,
        [int]$TimeoutSeconds = 300
    )
    
    if ($DryRun) {
        Write-Host "üèÉ Skipping wait in dry run mode" -ForegroundColor Yellow
        return $true
    }
    
    Write-Host "‚è≥ Waiting for $DeploymentName to be ready..." -ForegroundColor Yellow
    
    $timeout = [datetime]::Now.AddSeconds($TimeoutSeconds)
    while ([datetime]::Now -lt $timeout) {
        try {
            $ready = kubectl get statefulset $DeploymentName -n $Namespace -o jsonpath='{.status.readyReplicas}' 2>$null
            $desired = kubectl get statefulset $DeploymentName -n $Namespace -o jsonpath='{.spec.replicas}' 2>$null
            
            if ($ready -eq $desired -and $ready -gt 0) {
                Write-Host "‚úÖ $DeploymentName is ready!" -ForegroundColor Green
                return $true
            }
        }
        catch {
            # Deployment might not exist yet
        }
        
        Start-Sleep -Seconds 10
    }
    
    Write-Host "‚ö†Ô∏è Timeout waiting for $DeploymentName" -ForegroundColor Yellow
    return $false
}

# Function to verify services
function Test-Services {
    Write-Host "üîç Verifying services..." -ForegroundColor Cyan
    
    if (-not $SkipRedis) {
        Write-Host "Testing Redis cluster..." -ForegroundColor Yellow
        $redisTest = kubectl exec -n offerkiller-data redis-cluster-0 -- redis-cli ping 2>$null
        if ($redisTest -eq "PONG") {
            Write-Host "‚úÖ Redis cluster is responding" -ForegroundColor Green
        } else {
            Write-Host "‚ö†Ô∏è Redis cluster is not responding" -ForegroundColor Yellow
        }
    }
    
    if (-not $SkipRabbitMQ) {
        Write-Host "Testing RabbitMQ cluster..." -ForegroundColor Yellow
        $rmqTest = kubectl exec -n offerkiller-data rabbitmq-ha-0 -- rabbitmqctl cluster_status 2>$null
        if ($LASTEXITCODE -eq 0) {
            Write-Host "‚úÖ RabbitMQ cluster is operational" -ForegroundColor Green
        } else {
            Write-Host "‚ö†Ô∏è RabbitMQ cluster has issues" -ForegroundColor Yellow
        }
    }
    
    if (-not $SkipVectorDB) {
        Write-Host "Testing Vector Database..." -ForegroundColor Yellow
        $vectorTest = kubectl exec -n offerkiller-data vector-database-0 -- curl -s http://localhost:8000/api/v1/heartbeat 2>$null
        if ($vectorTest) {
            Write-Host "‚úÖ Vector Database is responding" -ForegroundColor Green
        } else {
            Write-Host "‚ö†Ô∏è Vector Database is not responding" -ForegroundColor Yellow
        }
    }
}

# Main deployment logic
if (-not (Test-Prerequisites)) {
    exit 1
}

# Navigate to project root
$ProjectRoot = Split-Path -Parent $PSScriptRoot
Set-Location $ProjectRoot

Write-Host "üìÇ Working directory: $PWD" -ForegroundColor Yellow
Write-Host "üéØ Target environment: $Environment" -ForegroundColor Yellow

# Deploy Redis Cluster
if (-not $SkipRedis) {
    $valuesFile = "infrastructure/helm/values/$Environment/redis-cluster.yaml"
    $success = Deploy-HelmChart -ChartPath "infrastructure/helm/charts/redis-cluster" `
                               -ReleaseName "redis-cluster" `
                               -Namespace "offerkiller-data" `
                               -ValuesFile $valuesFile `
                               -Description "Redis Cluster"
    
    if ($success) {
        Wait-ForDeployment -Namespace "offerkiller-data" -DeploymentName "redis-cluster"
    } else {
        Write-Host "‚ùå Redis deployment failed" -ForegroundColor Red
        exit 1
    }
}

# Deploy RabbitMQ Cluster
if (-not $SkipRabbitMQ) {
    $valuesFile = "infrastructure/helm/values/$Environment/rabbitmq-ha.yaml"
    $success = Deploy-HelmChart -ChartPath "infrastructure/helm/charts/rabbitmq-ha" `
                               -ReleaseName "rabbitmq-ha" `
                               -Namespace "offerkiller-data" `
                               -ValuesFile $valuesFile `
                               -Description "RabbitMQ HA Cluster"
    
    if ($success) {
        Wait-ForDeployment -Namespace "offerkiller-data" -DeploymentName "rabbitmq-ha"
    } else {
        Write-Host "‚ùå RabbitMQ deployment failed" -ForegroundColor Red
        exit 1
    }
}

# Deploy Vector Database
if (-not $SkipVectorDB) {
    $valuesFile = "infrastructure/helm/values/$Environment/vector-database.yaml"
    $success = Deploy-HelmChart -ChartPath "infrastructure/helm/charts/vector-database" `
                               -ReleaseName "vector-database" `
                               -Namespace "offerkiller-data" `
                               -ValuesFile $valuesFile `
                               -Description "Vector Database"
    
    if ($success) {
        Wait-ForDeployment -Namespace "offerkiller-data" -DeploymentName "vector-database"
    } else {
        Write-Host "‚ùå Vector Database deployment failed" -ForegroundColor Red
        exit 1
    }
}

# Verify deployments
Write-Host "`nüîç Deployment Status:" -ForegroundColor Yellow
kubectl get pods -n offerkiller-data
kubectl get svc -n offerkiller-data
kubectl get pvc -n offerkiller-data

# Test services
Test-Services

Write-Host "`nüéâ Data layer deployment completed!" -ForegroundColor Green

# Get access information
if (-not $DryRun) {
    $minikubeIP = try { minikube ip } catch { "localhost" }
    
    Write-Host "`nüìù Access Information:" -ForegroundColor Yellow
    Write-Host "   Redis Cluster:     $minikubeIP:30379" -ForegroundColor Cyan
    Write-Host "   RabbitMQ Mgmt:     http://$minikubeIP:31672 (offerkilleruser/rabbitmq123change)" -ForegroundColor Cyan
    Write-Host "   Vector Database:   http://$minikubeIP:30800" -ForegroundColor Cyan
    
    Write-Host "`nüîß Useful Commands:" -ForegroundColor Yellow
    Write-Host "   kubectl get pods -n offerkiller-data" -ForegroundColor White
    Write-Host "   kubectl logs -f statefulset/redis-cluster -n offerkiller-data" -ForegroundColor White
    Write-Host "   kubectl logs -f statefulset/rabbitmq-ha -n offerkiller-data" -ForegroundColor White
    Write-Host "   kubectl logs -f deployment/vector-database -n offerkiller-data" -ForegroundColor White
}

Write-Host "`nüöÄ Ready for application deployment!" -ForegroundColor Green
```

### 5.4 Linux VM: Deploy the Data Layer Services

Now let's execute the deployment step by step:

#### 5.4.1 Copy Latest Code to Linux VM

**On Windows, commit and push the changes:**

```powershell
cd E:\OfferKiller

# Add all new files
git add .

# Commit the changes
git commit -m "Add Helm charts for Redis cluster, RabbitMQ HA, and Vector Database

- Complete Redis cluster configuration with Sentinel for HA
- RabbitMQ cluster with management UI and HA policies  
- ChromaDB vector database with backup and scaling
- Environment-specific values for dev/staging/prod
- Comprehensive deployment scripts for Windows and Linux
- Network policies and security configurations
- Monitoring and health check configurations"

# Push to GitHub
git push origin main
```

**On Linux VM, pull the latest changes:**

```bash
# SSH into your Linux VM
cd ~/offerkiller

# Pull latest changes
git pull origin main

# Make deployment scripts executable
chmod +x scripts/deploy-data-layer.sh
chmod +x infrastructure/helm/scripts/*.sh

# Verify Helm charts are present
ls -la infrastructure/helm/charts/
```

#### 5.4.2 Install Required Helm Repositories

```bash
# Add required Helm repositories for dependencies
helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo add stable https://charts.helm.sh/stable
helm repo update

# Verify repositories
helm repo list
```

#### 5.4.3 Create Namespace for Data Layer

```bash
# Create namespace for data layer services
kubectl create namespace offerkiller-data

# Label namespace for Istio injection
kubectl label namespace offerkiller-data istio-injection=enabled

# Verify namespace creation
kubectl get namespaces | grep offerkiller
```

#### 5.4.4 Deploy Redis Cluster

```bash
# Deploy Redis cluster first (since other services may depend on it)
helm upgrade --install redis-cluster \
  infrastructure/helm/charts/redis-cluster \
  -n offerkiller-data \
  -f infrastructure/helm/values/development/redis-cluster.yaml \
  --wait --timeout 10m

# Verify Redis deployment
kubectl get pods -n offerkiller-data | grep redis
kubectl get svc -n offerkiller-data | grep redis

# Wait for Redis to be ready
kubectl wait --for=condition=ready pod -l app=redis-cluster -n offerkiller-data --timeout=300s
```

#### 5.4.5 Test Redis Cluster

```bash
# Test Redis connectivity
kubectl exec -n offerkiller-data redis-cluster-0 -- redis-cli ping

# Test cluster status
kubectl exec -n offerkiller-data redis-cluster-0 -- redis-cli cluster info

# Test data persistence
kubectl exec -n offerkiller-data redis-cluster-0 -- redis-cli set test-key "hello-world"
kubectl exec -n offerkiller-data redis-cluster-0 -- redis-cli get test-key
```

#### 5.4.6 Deploy RabbitMQ HA Cluster

```bash
# Deploy RabbitMQ cluster
helm upgrade --install rabbitmq-ha \
  infrastructure/helm/charts/rabbitmq-ha \
  -n offerkiller-data \
  -f infrastructure/helm/values/development/rabbitmq-ha.yaml \
  --wait --timeout 10m

# Verify RabbitMQ deployment
kubectl get pods -n offerkiller-data | grep rabbitmq
kubectl get svc -n offerkiller-data | grep rabbitmq

# Wait for RabbitMQ to be ready
kubectl wait --for=condition=ready pod -l app=rabbitmq-ha -n offerkiller-data --timeout=300s
```

#### 5.4.7 Test RabbitMQ Cluster

```bash
# Test RabbitMQ cluster status
kubectl exec -n offerkiller-data rabbitmq-ha-0 -- rabbitmqctl cluster_status

# Test management interface access
kubectl port-forward -n offerkiller-data svc/rabbitmq-ha 15672:15672 &

# In another terminal or use curl
curl -u offerkilleruser:rabbitmq123change http://localhost:15672/api/overview

# Test queue operations
kubectl exec -n offerkiller-data rabbitmq-ha-0 -- rabbitmqadmin declare queue name=test-queue
kubectl exec -n offerkiller-data rabbitmq-ha-0 -- rabbitmqadmin publish exchange=amq.default routing_key=test-queue payload="hello world"
kubectl exec -n offerkiller-data rabbitmq-ha-0 -- rabbitmqadmin get queue=test-queue
```

#### 5.4.8 Deploy Vector Database

```bash
# Deploy Vector Database
helm upgrade --install vector-database \
  infrastructure/helm/charts/vector-database \
  -n offerkiller-data \
  -f infrastructure/helm/values/development/vector-database.yaml \
  --wait --timeout 10m

# Verify Vector Database deployment
kubectl get pods -n offerkiller-data | grep vector
kubectl get svc -n offerkiller-data | grep vector

# Wait for Vector Database to be ready
kubectl wait --for=condition=ready pod -l app=vector-database -n offerkiller-data --timeout=300s
```

#### 5.4.9 Test Vector Database

```bash
# Test ChromaDB API
kubectl exec -n offerkiller-data vector-database-0 -- curl -s http://localhost:8000/api/v1/heartbeat

# Test collection creation
kubectl port-forward -n offerkiller-data svc/vector-database 8000:8000 &

# Create test collection (in another terminal)
curl -X POST "http://localhost:8000/api/v1/collections" \
  -H "Content-Type: application/json" \
  -d '{"name": "test-collection", "metadata": {"description": "Test collection"}}'

# List collections
curl -X GET "http://localhost:8000/api/v1/collections"
```

### 5.5 Validation and Testing

#### 5.5.1 Comprehensive Service Health Check

```bash
# Create comprehensive health check script
cat > scripts/health-check-data-layer.sh << 'EOF'
#!/bin/bash

echo "üîç OfferKiller Data Layer Health Check"
echo "======================================"

# Check namespace
echo "üì¶ Checking namespace..."
kubectl get namespace offerkiller-data

# Check all pods
echo "üìã Pod Status:"
kubectl get pods -n offerkiller-data -o wide

# Check services
echo "üåê Service Status:"
kubectl get svc -n offerkiller-data

# Check persistent volumes
echo "üíæ Storage Status:"
kubectl get pvc -n offerkiller-data

# Test Redis
echo "üî¥ Testing Redis Cluster..."
redis_result=$(kubectl exec -n offerkiller-data redis-cluster-0 -- redis-cli ping 2>/dev/null)
if [ "$redis_result" = "PONG" ]; then
    echo "‚úÖ Redis is responding"
    kubectl exec -n offerkiller-data redis-cluster-0 -- redis-cli cluster nodes | head -3
else
    echo "‚ùå Redis is not responding"
fi

# Test RabbitMQ
echo "üê∞ Testing RabbitMQ Cluster..."
rabbitmq_result=$(kubectl exec -n offerkiller-data rabbitmq-ha-0 -- rabbitmqctl status --quiet 2>/dev/null)
if [ $? -eq 0 ]; then
    echo "‚úÖ RabbitMQ is operational"
    kubectl exec -n offerkiller-data rabbitmq-ha-0 -- rabbitmqctl cluster_status --quiet | grep "Running nodes"
else
    echo "‚ùå RabbitMQ has issues"
fi

# Test Vector Database
echo "üß† Testing Vector Database..."
vector_result=$(kubectl exec -n offerkiller-data vector-database-0 -- curl -s http://localhost:8000/api/v1/heartbeat 2>/dev/null)
if [ -n "$vector_result" ]; then
    echo "‚úÖ Vector Database is responding"
    echo "Response: $vector_result"
else
    echo "‚ùå Vector Database is not responding"
fi

# Check resource usage
echo "üìä Resource Usage:"
kubectl top pods -n offerkiller-data 2>/dev/null || echo "Metrics server not available"

echo "‚úÖ Health check completed!"
EOF

chmod +x scripts/health-check-data-layer.sh
./scripts/health-check-data-layer.sh
```

#### 5.5.2 Performance Testing

```bash
# Create performance test script for each service
cat > scripts/performance-test-data-layer.sh << 'EOF'
#!/bin/bash

echo "‚ö° OfferKiller Data Layer Performance Tests"
echo "=========================================="

# Redis Performance Test
echo "üî¥ Redis Performance Test..."
kubectl exec -n offerkiller-data redis-cluster-0 -- redis-cli --latency-history -i 1 | head -10 &
redis_perf_pid=$!

kubectl exec -n offerkiller-data redis-cluster-0 -- redis-benchmark -t set,get -n 10000 -c 50 -d 64 --csv

kill $redis_perf_pid 2>/dev/null

# RabbitMQ Performance Test
echo "üê∞ RabbitMQ Performance Test..."
kubectl exec -n offerkiller-data rabbitmq-ha-0 -- rabbitmq-perf-test -x 1 -y 2 -u "test-queue" -s 1000 -m 1000

# Vector Database Performance Test
echo "üß† Vector Database Performance Test..."
kubectl port-forward -n offerkiller-data svc/vector-database 8000:8000 &
port_forward_pid=$!

sleep 5

# Simple API response time test
for i in {1..10}; do
    start_time=$(date +%s%N)
    curl -s http://localhost:8000/api/v1/heartbeat > /dev/null
    end_time=$(date +%s%N)
    response_time=$(( (end_time - start_time) / 1000000 ))
    echo "Vector DB Response $i: ${response_time}ms"
done

kill $port_forward_pid 2>/dev/null

echo "‚úÖ Performance tests completed!"
EOF

chmod +x scripts/performance-test-data-layer.sh
```

#### 5.5.3 Integration Testing

```bash
# Create integration test script
cat > scripts/integration-test-data-layer.sh << 'EOF'
#!/bin/bash

echo "üîó OfferKiller Data Layer Integration Tests"
echo "==========================================="

# Test cross-service connectivity
echo "üì° Testing service-to-service connectivity..."

# Test Redis from RabbitMQ pod
echo "Testing Redis from RabbitMQ pod..."
kubectl exec -n offerkiller-data rabbitmq-ha-0 -- nc -zv redis-cluster.offerkiller-data.svc.cluster.local 6379

# Test Vector DB from Redis pod
echo "Testing Vector DB from Redis pod..."
kubectl exec -n offerkiller-data redis-cluster-0 -- nc -zv vector-database.offerkiller-data.svc.cluster.local 8000

# Test RabbitMQ from Vector DB pod
echo "Testing RabbitMQ from Vector DB pod..."
kubectl exec -n offerkiller-data vector-database-0 -- nc -zv rabbitmq-ha.offerkiller-data.svc.cluster.local 5672

# Test DNS resolution
echo "üåê Testing DNS resolution..."
kubectl exec -n offerkiller-data redis-cluster-0 -- nslookup rabbitmq-ha.offerkiller-data.svc.cluster.local
kubectl exec -n offerkiller-data rabbitmq-ha-0 -- nslookup vector-database.offerkiller-data.svc.cluster.local

# Test service discovery via Nacos
echo "üîç Testing service registration with Nacos..."
nacos_url="http://nacos.offerkiller-system.svc.cluster.local:8848"
kubectl exec -n offerkiller-data redis-cluster-0 -- curl -s "$nacos_url/nacos/v1/ns/catalog/services"

echo "‚úÖ Integration tests completed!"
EOF

chmod +x scripts/integration-test-data-layer.sh
./scripts/integration-test-data-layer.sh
```

### 5.6 Windows: Configure Access and Monitoring

#### 5.6.1 Configure Port Forwards for Development

**Create file: `E:\OfferKiller\scripts\setup-dev-access.ps1`**

```powershell
# OfferKiller Development Access Setup Script
Write-Host "üîó Setting up development access to data layer services..." -ForegroundColor Green

# Function to start port forward in background
function Start-PortForward {
    param(
        [string]$Service,
        [string]$Namespace,
        [string]$LocalPort,
        [string]$RemotePort,
        [string]$Description
    )
    
    Write-Host "üöÄ Starting port forward for $Description..." -ForegroundColor Cyan
    Write-Host "   Local: localhost:$LocalPort -> Remote: $Service:$RemotePort" -ForegroundColor Yellow
    
    $job = Start-Job -ScriptBlock {
        param($Service, $Namespace, $LocalPort, $RemotePort)
        kubectl port-forward -n $Namespace "svc/$Service" "${LocalPort}:${RemotePort}"
    } -ArgumentList $Service, $Namespace, $LocalPort, $RemotePort
    
    return $job
}

# Start port forwards
$jobs = @()

# Redis Cluster
$jobs += Start-PortForward -Service "redis-cluster" -Namespace "offerkiller-data" -LocalPort "6379" -RemotePort "6379" -Description "Redis Cluster"

# RabbitMQ Management
$jobs += Start-PortForward -Service "rabbitmq-ha" -Namespace "offerkiller-data" -LocalPort "15672" -RemotePort "15672" -Description "RabbitMQ Management UI"

# RabbitMQ AMQP
$jobs += Start-PortForward -Service "rabbitmq-ha" -Namespace "offerkiller-data" -LocalPort "5672" -RemotePort "5672" -Description "RabbitMQ AMQP"

# Vector Database
$jobs += Start-PortForward -Service "vector-database" -Namespace "offerkiller-data" -LocalPort "8000" -RemotePort "8000" -Description "Vector Database API"

Write-Host "`n‚úÖ Port forwards started!" -ForegroundColor Green
Write-Host "`nüìù Access Information:" -ForegroundColor Yellow
Write-Host "   Redis:             localhost:6379" -ForegroundColor Cyan
Write-Host "   RabbitMQ AMQP:     localhost:5672" -ForegroundColor Cyan
Write-Host "   RabbitMQ Mgmt:     http://localhost:15672 (offerkilleruser/rabbitmq123change)" -ForegroundColor Cyan
Write-Host "   Vector Database:   http://localhost:8000" -ForegroundColor Cyan

Write-Host "`nüîß Test Commands:" -ForegroundColor Yellow
Write-Host "   Redis:     redis-cli -h localhost -p 6379 ping" -ForegroundColor White
Write-Host "   RabbitMQ:  Open http://localhost:15672 in browser" -ForegroundColor White
Write-Host "   Vector:    curl http://localhost:8000/api/v1/heartbeat" -ForegroundColor White

Write-Host "`n‚ö†Ô∏è  Press Ctrl+C to stop all port forwards" -ForegroundColor Yellow

# Wait for user input to stop
try {
    while ($true) {
        Start-Sleep -Seconds 5
        
        # Check if any jobs failed
        $failedJobs = $jobs | Where-Object { $_.State -eq "Failed" }
        if ($failedJobs) {
            Write-Host "‚ö†Ô∏è Some port forwards failed. Restarting..." -ForegroundColor Yellow
            $failedJobs | ForEach-Object { Remove-Job $_ -Force }
            # Restart logic could be added here
        }
    }
}
finally {
    Write-Host "`nüõë Stopping all port forwards..." -ForegroundColor Yellow
    $jobs | ForEach-Object { Stop-Job $_ -Force; Remove-Job $_ -Force }
    Write-Host "‚úÖ All port forwards stopped." -ForegroundColor Green
}
```

#### 5.6.2 Create Monitoring Dashboard Configuration

**Create file: `E:\OfferKiller\infrastructure\monitoring\grafana\dashboards\data-layer-dashboard.json`**

```json
{
  "dashboard": {
    "id": null,
    "title": "OfferKiller Data Layer Dashboard",
    "tags": ["offerkiller", "data-layer", "redis", "rabbitmq", "vector-db"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "Redis Cluster Status",
        "type": "stat",
        "targets": [
          {
            "expr": "redis_connected_clients",
            "legendFormat": "Connected Clients"
          },
          {
            "expr": "redis_keyspace_hits_total",
            "legendFormat": "Cache Hits"
          },
          {
            "expr": "redis_keyspace_misses_total", 
            "legendFormat": "Cache Misses"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0}
      },
      {
        "id": 2,
        "title": "RabbitMQ Cluster Status",
        "type": "stat",
        "targets": [
          {
            "expr": "rabbitmq_global_messages_ready",
            "legendFormat": "Messages Ready"
          },
          {
            "expr": "rabbitmq_global_messages_unacknowledged",
            "legendFormat": "Unacked Messages"
          },
          {
            "expr": "rabbitmq_connections",
            "legendFormat": "Connections"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0}
      },
      {
        "id": 3,
        "title": "Vector Database Performance",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(chromadb_requests_total[5m])",
            "legendFormat": "Requests/sec"
          },
          {
            "expr": "chromadb_request_duration_seconds",
            "legendFormat": "Response Time"
          }
        ],
        "gridPos": {"h": 8, "w": 24, "x": 0, "y": 8}
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "refresh": "30s"
  }
}
```

### 5.7 Create Backup and Recovery Procedures

#### 5.7.1 Create Backup Script

**Create file: `E:\OfferKiller\scripts\backup-data-layer.sh`**

```bash
#!/bin/bash

# OfferKiller Data Layer Backup Script

set -e

BACKUP_DATE=$(date +"%Y%m%d_%H%M%S")
BACKUP_DIR="/tmp/offerkiller-backups/$BACKUP_DATE"
NAMESPACE="offerkiller-data"

echo "üì¶ OfferKiller Data Layer Backup - $BACKUP_DATE"
echo "==============================================="

# Create backup directory
mkdir -p "$BACKUP_DIR"

# Function to backup Redis
backup_redis() {
    echo "üî¥ Backing up Redis cluster..."
    
    # Create Redis dump
    kubectl exec -n $NAMESPACE redis-cluster-0 -- redis-cli BGSAVE
    
    # Wait for backup to complete
    while [ "$(kubectl exec -n $NAMESPACE redis-cluster-0 -- redis-cli LASTSAVE)" = "$(kubectl exec -n $NAMESPACE redis-cluster-0 -- redis-cli LASTSAVE)" ]; do
        sleep 1
    done
    
    # Copy dump file
    kubectl cp $NAMESPACE/redis-cluster-0:/data/dump.rdb "$BACKUP_DIR/redis-dump.rdb"
    
    # Export cluster configuration
    kubectl get configmap redis-cluster-config -n $NAMESPACE -o yaml > "$BACKUP_DIR/redis-config.yaml"
    
    echo "‚úÖ Redis backup completed"
}

# Function to backup RabbitMQ
backup_rabbitmq() {
    echo "üê∞ Backing up RabbitMQ cluster..."
    
    # Export definitions
    kubectl exec -n $NAMESPACE rabbitmq-ha-0 -- rabbitmqctl export_definitions /tmp/rabbitmq-definitions.json
    kubectl cp $NAMESPACE/rabbitmq-ha-0:/tmp/rabbitmq-definitions.json "$BACKUP_DIR/rabbitmq-definitions.json"
    
    # Backup persistent data
    kubectl exec -n $NAMESPACE rabbitmq-ha-0 -- tar -czf /tmp/rabbitmq-data.tar.gz /var/lib/rabbitmq/
    kubectl cp $NAMESPACE/rabbitmq-ha-0:/tmp/rabbitmq-data.tar.gz "$BACKUP_DIR/rabbitmq-data.tar.gz"
    
    # Export configuration
    kubectl get configmap rabbitmq-ha-config -n $NAMESPACE -o yaml > "$BACKUP_DIR/rabbitmq-config.yaml"
    
    echo "‚úÖ RabbitMQ backup completed"
}

# Function to backup Vector Database
backup_vector_database() {
    echo "üß† Backing up Vector Database..."
    
    # Backup ChromaDB data
    kubectl exec -n $NAMESPACE vector-database-0 -- tar -czf /tmp/chromadb-data.tar.gz /chroma/chroma/
    kubectl cp $NAMESPACE/vector-database-0:/tmp/chromadb-data.tar.gz "$BACKUP_DIR/chromadb-data.tar.gz"
    
    # Export collections metadata
    kubectl port-forward -n $NAMESPACE svc/vector-database 8000:8000 &
    port_forward_pid=$!
    
    sleep 5
    
    # Export collections list
    curl -s http://localhost:8000/api/v1/collections > "$BACKUP_DIR/chromadb-collections.json"
    
    kill $port_forward_pid 2>/dev/null
    
    # Export configuration
    kubectl get configmap vector-database-config -n $NAMESPACE -o yaml > "$BACKUP_DIR/vector-database-config.yaml"
    
    echo "‚úÖ Vector Database backup completed"
}

# Function to backup Kubernetes resources
backup_kubernetes_resources() {
    echo "‚ò∏Ô∏è Backing up Kubernetes resources..."
    
    # Backup all resources in the namespace
    kubectl get all -n $NAMESPACE -o yaml > "$BACKUP_DIR/kubernetes-resources.yaml"
    kubectl get pvc -n $NAMESPACE -o yaml > "$BACKUP_DIR/persistent-volume-claims.yaml"
    kubectl get secrets -n $NAMESPACE -o yaml > "$BACKUP_DIR/secrets.yaml"
    kubectl get configmaps -n $NAMESPACE -o yaml > "$BACKUP_DIR/configmaps.yaml"
    
    echo "‚úÖ Kubernetes resources backup completed"
}

# Execute backups
backup_redis
backup_rabbitmq
backup_vector_database
backup_kubernetes_resources

# Create backup metadata
cat > "$BACKUP_DIR/backup-metadata.json" << EOF
{
  "backup_date": "$BACKUP_DATE",
  "namespace": "$NAMESPACE",
  "services": ["redis-cluster", "rabbitmq-ha", "vector-database"],
  "backup_size": "$(du -sh $BACKUP_DIR | cut -f1)",
  "kubernetes_version": "$(kubectl version --short --client)",
  "cluster_info": "$(kubectl cluster-info | head -1)"
}
EOF

# Compress backup
tar -czf "/tmp/offerkiller-backup-$BACKUP_DATE.tar.gz" -C "/tmp/offerkiller-backups" "$BACKUP_DATE"

echo "üì¶ Backup completed: /tmp/offerkiller-backup-$BACKUP_DATE.tar.gz"
echo "üìä Backup size: $(du -sh /tmp/offerkiller-backup-$BACKUP_DATE.tar.gz | cut -f1)"

# Cleanup temporary directory
rm -rf "$BACKUP_DIR"

echo "‚úÖ Backup process completed successfully!"
```

### 5.8 Troubleshooting Guide

#### 5.8.1 Common Issues and Solutions

**Create file: `E:\OfferKiller\docs\troubleshooting\data-layer-issues.md`**

```markdown
# Data Layer Troubleshooting Guide

## Common Issues and Solutions

### 1. Redis Cluster Issues

#### Problem: Redis nodes cannot form cluster
```bash
kubectl logs -n offerkiller-data redis-cluster-0
```

**Solution**: Check network policies and ensure nodes can communicate
```bash
# Check network connectivity between nodes
kubectl exec -n offerkiller-data redis-cluster-0 -- redis-cli -h redis-cluster-1.redis-cluster 6379 ping

# Reset cluster if needed
kubectl exec -n offerkiller-data redis-cluster-0 -- redis-cli FLUSHALL
kubectl exec -n offerkiller-data redis-cluster-0 -- redis-cli CLUSTER RESET
```

#### Problem: Redis memory issues
```bash
kubectl top pods -n offerkiller-data | grep redis
```

**Solution**: Adjust memory limits and configuration
```bash
# Check memory usage
kubectl exec -n offerkiller-data redis-cluster-0 -- redis-cli INFO memory

# Update resource limits
kubectl patch statefulset redis-cluster -n offerkiller-data -p '{"spec":{"template":{"spec":{"containers":[{"name":"redis","resources":{"limits":{"memory":"1Gi"}}}]}}}}'
```

### 2. RabbitMQ Cluster Issues

#### Problem: RabbitMQ cluster split-brain
```bash
kubectl exec -n offerkiller-data rabbitmq-ha-0 -- rabbitmqctl cluster_status
```

**Solution**: Force cluster reformation
```bash
# Stop all nodes except one
kubectl scale statefulset rabbitmq-ha -n offerkiller-data --replicas=1

# Wait for single node to be ready
kubectl wait --for=condition=ready pod rabbitmq-ha-0 -n offerkiller-data --timeout=300s

# Scale back up
kubectl scale statefulset rabbitmq-ha -n offerkiller-data --replicas=3
```

#### Problem: RabbitMQ management UI not accessible
```bash
kubectl get svc -n offerkiller-data | grep rabbitmq
```

**Solution**: Check service configuration and port forwards
```bash
# Check service endpoints
kubectl get endpoints rabbitmq-ha -n offerkiller-data

# Test management plugin
kubectl exec -n offerkiller-data rabbitmq-ha-0 -- rabbitmq-plugins list | grep management

# Enable management plugin if needed
kubectl exec -n offerkiller-data rabbitmq-ha-0 -- rabbitmq-plugins enable rabbitmq_management
```

### 3. Vector Database Issues

#### Problem: ChromaDB not responding
```bash
kubectl logs -n offerkiller-data vector-database-0
```

**Solution**: Check storage and restart if needed
```bash
# Check disk space
kubectl exec -n offerkiller-data vector-database-0 -- df -h

# Check ChromaDB process
kubectl exec -n offerkiller-data vector-database-0 -- ps aux | grep chroma

# Restart if needed
kubectl rollout restart deployment/vector-database -n offerkiller-data
```

#### Problem: Vector collections not accessible
```bash
curl -X GET "http://localhost:8000/api/v1/collections"
```

**Solution**: Verify data persistence and reinitialize collections
```bash
# Check persistent volume
kubectl get pvc -n offerkiller-data | grep vector

# Check data directory
kubectl exec -n offerkiller-data vector-database-0 -- ls -la /chroma/chroma/

# Recreate collections if needed
curl -X POST "http://localhost:8000/api/v1/collections" \
  -H "Content-Type: application/json" \
  -d '{"name": "resumes", "metadata": {"description": "Resume embeddings"}}'
```

### 4. Storage Issues

#### Problem: Persistent volumes not mounting
```bash
kubectl get pvc -n offerkiller-data
kubectl describe pvc redis-cluster-data-redis-cluster-0 -n offerkiller-data
```

**Solution**: Check storage class and provisioner
```bash
# Check storage class
kubectl get storageclass

# Check provisioner
kubectl get pods -n kube-system | grep provisioner

# Recreate PVC if needed
kubectl delete pvc redis-cluster-data-redis-cluster-0 -n offerkiller-data
kubectl patch statefulset redis-cluster -n offerkiller-data -p '{"spec":{"volumeClaimTemplates":[{"metadata":{"name":"data"},"spec":{"storageClassName":"standard","accessModes":["ReadWriteOnce"],"resources":{"requests":{"storage":"8Gi"}}}}]}}'
```

### 5. Network and Connectivity Issues

#### Problem: Services cannot reach each other
```bash
kubectl get networkpolicy -n offerkiller-data
```

**Solution**: Review and adjust network policies
```bash
# Test connectivity
kubectl exec -n offerkiller-data redis-cluster-0 -- nc -zv rabbitmq-ha.offerkiller-data.svc.cluster.local 5672

# Temporarily disable network policies for testing
kubectl delete networkpolicy --all -n offerkiller-data

# Check DNS resolution
kubectl exec -n offerkiller-data redis-cluster-0 -- nslookup vector-database.offerkiller-data.svc.cluster.local
```

### 6. Performance Issues

#### Problem: High latency or poor performance
```bash
kubectl top pods -n offerkiller-data
kubectl top nodes
```

**Solution**: Scale resources and optimize configuration
```bash
# Scale up resources
kubectl patch statefulset redis-cluster -n offerkiller-data -p '{"spec":{"template":{"spec":{"containers":[{"name":"redis","resources":{"requests":{"cpu":"500m","memory":"512Mi"},"limits":{"cpu":"1000m","memory":"1Gi"}}}]}}}}'

# Enable HPA for auto-scaling
kubectl autoscale deployment vector-database -n offerkiller-data --cpu-percent=70 --min=2 --max=5
```

## Diagnostic Commands

### General Health Check
```bash
# Overall status
kubectl get all -n offerkiller-data

# Resource usage
kubectl top pods -n offerkiller-data

# Events
kubectl get events -n offerkiller-data --sort-by=.metadata.creationTimestamp
```

### Service-Specific Diagnostics
```bash
# Redis diagnostics
kubectl exec -n offerkiller-data redis-cluster-0 -- redis-cli INFO
kubectl exec -n offerkiller-data redis-cluster-0 -- redis-cli CLUSTER NODES

# RabbitMQ diagnostics
kubectl exec -n offerkiller-data rabbitmq-ha-0 -- rabbitmqctl status
kubectl exec -n offerkiller-data rabbitmq-ha-0 -- rabbitmqctl cluster_status

# Vector Database diagnostics
kubectl exec -n offerkiller-data vector-database-0 -- curl -s http://localhost:8000/api/v1/heartbeat
kubectl logs -n offerkiller-data vector-database-0 --tail=100
```

### Log Analysis
```bash
# Tail logs for all services
kubectl logs -f statefulset/redis-cluster -n offerkiller-data
kubectl logs -f statefulset/rabbitmq-ha -n offerkiller-data
kubectl logs -f deployment/vector-database -n offerkiller-data

# Search for errors
kubectl logs statefulset/redis-cluster -n offerkiller-data | grep -i error
kubectl logs statefulset/rabbitmq-ha -n offerkiller-data | grep -i error
```

## Recovery Procedures

### Complete Reset
```bash
# Delete all resources
helm uninstall redis-cluster -n offerkiller-data
helm uninstall rabbitmq-ha -n offerkiller-data
helm uninstall vector-database -n offerkiller-data

# Clean up persistent data
kubectl delete pvc --all -n offerkiller-data

# Redeploy
./scripts/deploy-data-layer.sh --environment development
```

### Backup and Restore
```bash
# Create backup
./scripts/backup-data-layer.sh

# Restore from backup
./scripts/restore-data-layer.sh /path/to/backup.tar.gz
```
```

## Success Criteria

Your Step 5 implementation is **successful** when you can confirm:

### ‚úÖ **Redis Cluster Operational**
- [ ] Redis cluster with multiple nodes running and healthy
- [ ] Redis Sentinel monitoring and failover capability
- [ ] Cache operations working (SET/GET commands)
- [ ] Cluster status shows all nodes connected
- [ ] Performance metrics being collected

### ‚úÖ **RabbitMQ HA Cluster Functional**
- [ ] RabbitMQ cluster with HA policies configured
- [ ] Management UI accessible and functional
- [ ] Queue operations working (publish/consume)
- [ ] Cluster status shows all nodes joined
- [ ] Message persistence enabled

### ‚úÖ **Vector Database Ready**
- [ ] ChromaDB instance running and accessible
- [ ] API endpoints responding correctly
- [ ] Collections can be created and queried
- [ ] Persistent storage mounted and working
- [ ] Health checks passing

### ‚úÖ **High Availability Configured**
- [ ] Pod disruption budgets in place
- [ ] Anti-affinity rules distributing pods across nodes
- [ ] Persistent volumes for data durability
- [ ] Backup procedures tested and working
- [ ] Monitoring and alerting configured

### ‚úÖ **Integration Ready**
- [ ] Services discoverable via DNS
- [ ] Network policies allowing appropriate traffic
- [ ] Service mesh integration (Istio) functional
- [ ] Connection from application namespace tested
- [ ] Configuration management via Nacos prepared

**Test Command to Verify Success:**
```bash
# Run comprehensive verification
./scripts/health-check-data-layer.sh && \
./scripts/integration-test-data-layer.sh && \
echo "‚úÖ Data layer is ready for application integration!"
```

## Next Steps

After completing Step 5 successfully:

1. **‚úÖ Step 1**: Docker development environment ‚úì
2. **‚úÖ Step 2**: Git monorepo structure ‚úì  
3. **‚úÖ Step 3**: GitHub Actions CI/CD pipeline ‚úì
4. **‚úÖ Step 4**: Kubernetes foundational services ‚úì
5. **‚úÖ Step 5**: Helm-deployed data layer services ‚úì
6. **‚û°Ô∏è Step 6**: MySQL database schemas and MyBatis data access layer

You now have a **complete high-availability data infrastructure** providing:
- **Distributed Caching**: Redis cluster for session storage and temporary data
- **Reliable Messaging**: RabbitMQ cluster for asynchronous communication
- **AI Vector Operations**: ChromaDB for embeddings and similarity search
- **Enterprise Features**: Backup, monitoring, scaling, and recovery capabilities
- **Production Readiness**: HA configuration with fault tolerance

Your microservices can now leverage this robust data layer for scalable, reliable operations!

## Notes for User Actions

### üñ•Ô∏è **Windows Tasks for You:**
1. **Review Configuration Files**: Check all Helm values files in `infrastructure/helm/values/`
2. **Test Development Access**: Run `scripts/setup-dev-access.ps1` to access services locally
3. **Verify Integration**: Ensure your development tools can connect to the data layer services

### üêß **Linux Tasks for You:**
1. **Execute Deployment**: Run `./scripts/deploy-data-layer.sh --environment development`
2. **Run Health Checks**: Execute `./scripts/health-check-data-layer.sh`
3. **Performance Testing**: Run `./scripts/performance-test-data-layer.sh`
4. **Create Backup**: Execute `./scripts/backup-data-layer.sh`

### üîß **Configuration Tasks:**
1. **Update Passwords**: Change default passwords in values files before production
2. **Resource Tuning**: Adjust CPU/memory based on your VM capacity
3. **Storage Configuration**: Verify storage classes match your cluster setup
4. **Monitoring Setup**: Configure Grafana dashboards for data layer monitoring

The data layer is now ready to support all OfferKiller applications with enterprise-grade reliability and performance!
